#! /usr/bin/env python
# Note that cfy is used rather than the client so that the user does not have
# to provide credentials, etc, to this script.
from __future__ import print_function
import argparse
from contextlib import contextmanager
import copy
import datetime
from functools import total_ordering
import hashlib
import json
import os
import shutil
import subprocess
import sys
import tempfile
import time
import urllib2
try:
    import yaml
except ImportError:
    yaml = None

BASE_MANAGER_PATCH_STORAGE_PATH = '/etc/cloudify/patches'
PATCH_REGISTRY_FILE_NAME = 'patch_registry.json'
PATCH_REGISTRY_PATH = os.path.join(
    BASE_MANAGER_PATCH_STORAGE_PATH, PATCH_REGISTRY_FILE_NAME
)
PATCHIFY_LOG_PATH = ""
SHA256SUM_SCRIPT = """#! /usr/bin/env python
import hashlib
import sys
target = sys.argv[1]
with open(target, 'rb') as data_handle:
   print(hashlib.sha256(data_handle.read()).hexdigest() + ' ' + target)
"""
PATCH_VERSIONS = {
    1: {
        'required': set(['manager_versions', 'community', 'premium',
                         'patch_version', 'affected_services', 'patches']),
        'recommended': set(['md5sums_before', 'md5sums_after']),
        'minor_versions': {
            1: {
                'required': set(['description']),
            },
        },
    },
    2: {
        'required': set(['manager_versions', 'community', 'premium',
                         'patch_version', 'description',
                         'affected_services', 'patches',
                         'sha256sums_before', 'sha256sums_after']),
        'minor_versions': {
            1: {
                'recommended': set(['node_types']),
            },
        },
    },
}
PATCHIFY_VERSION = (1, 3, 0)
OPTIONAL_SERVICES = {
    'cloudify-handler-runner',
    'cloudify-check-runner',
    'cloudify-consul-watcher',
    'cloudify-syncthing'
}
NODE_TYPES = ['manager', 'db', 'broker']
# This is set when the manager version is first retrieved to avoid an issue
# with checking whether we're on 5.0.5+ mid-service check when a service does
# not restart properly
_MANAGER_VERSION = None


class VersionError(Exception):
    pass


class UploadError(Exception):
    pass


class DownloadError(Exception):
    pass


class ClusterUnhealthy(Exception):
    pass


class ClusterLeaderChangeFailure(Exception):
    pass


class ClusterLeaderChangeToCurrent(Exception):
    pass


class SSHCheckFailure(Exception):
    pass


class ServiceHealthFailure(Exception):
    pass


class PatchNotFound(Exception):
    pass


class PatchRemovalBlocked(Exception):
    pass


class PatchDefinitionParseFailure(Exception):
    pass


class InvalidBackup(Exception):
    pass


class PatchRegistryParseFailure(Exception):
    pass


class HashCheckFailure(Exception):
    pass


class RegistryGenerationFailure(Exception):
    pass


class PatchApplyFailed(Exception):
    pass


class PatchAlreadyApplied(Exception):
    pass


class InvalidNodeType(Exception):
    pass


@total_ordering
class Version(object):
    def __init__(self, version_string, raw=None, edition=None):
        version = version_string.split('.')
        try:
            self._version = tuple(int(part) for part in version)
        except ValueError:
            raise VersionError(
                'Version string must contain only integer components. '
                'Found: {version}'.format(version=version),
            )
        self.edition = edition
        self.raw = raw

    def __repr__(self):
        return self._version

    def __str__(self):
        return '.'.join(str(part) for part in self._version)

    def __eq__(self, other):
        return self._version == other

    def __lt__(self, other):
        return self._version < other

    def __getitem__(self, index):
        return self._version[index]


def get_cloudify_workdir():
    base = os.environ.get('CFY_WORKDIR', os.path.expanduser('~'))
    return os.path.join(base, '.cloudify')


def abort_unless_profile_dir_exists():
    if not os.path.exists(get_cloudify_workdir()):
        sadly_say(
            'Patchify requires an active cfy profile for the manager you '
            'are attempting to patch.\n'
            'Please ensure this profile exists with `cfy profiles use`.\n'
            'FAILED TO START PATCHIFY.'
        )
        sys.exit(1)


def get_log_path():
    workdir = get_cloudify_workdir()
    path = os.path.join(workdir, 'logs', 'patchify.log')
    if yaml:
        cli_config_path = os.path.join(workdir, 'config.yaml')
        if os.path.exists(cli_config_path):
            with open(cli_config_path) as conf_handle:
                cli_conf = yaml.load(conf_handle.read())
            cli_log_dir = os.path.dirname(
                os.path.expanduser(cli_conf['logging']['filename'])
            )
            path = os.path.join(cli_log_dir, 'patchify.log')
    return path


def exit_with_sadness(message, action):
    sadly_say(message)
    sadly_say('FAILED {action}!'.format(action=action))
    sys.exit(1)


def say(message):
    print(message)
    log_to_file(message, 'INF')


def sadly_say(message):
    sys.stderr.write('%s\n' % message)
    log_to_file(message, 'ERR')


def log_to_file(message, level):
    lines = message.splitlines()
    if PATCHIFY_LOG_PATH:
        with open(PATCHIFY_LOG_PATH, 'a') as log_handle:
            for line in lines:
                log_handle.write(
                    '{timestamp} ({level}): {message}\n'.format(
                        timestamp=generate_timestamp(),
                        level=level,
                        message=line,
                    )
                )


def get_patchify_version_output():
    supported_def_vers = get_supported_definition_versions()

    return (
        'Patchify version {ver}. '
        'Supported patch definitions: {def_vers}.'
    ).format(
        ver=PATCHIFY_VERSION,
        def_vers=', '.join(supported_def_vers)
    )


def get_supported_definition_versions():
    supported_versions = []
    for major, schema in PATCH_VERSIONS.items():
        minor_versions = [0]
        minor_versions.extend(schema.get('minor_versions', {}).keys())
        for minor in minor_versions:
            supported_versions.append(
                '.'.join(str(v) for v in [major, minor, 0]),
            )
    return sorted(supported_versions)


def format_json(data):
    return json.dumps(data, indent=2, sort_keys=True)


def output_json(data):
    print(format_json(data))


def output_table(data, fields):
    field_lengths = []
    for field in fields:
        for entry in data:
            if isinstance(entry[field], list):
                entry[field] = ', '.join(entry[field])
        if data:
            field_length = max(
                2 + len(str(entry[field])) for entry in data
            )
        else:
            field_length = 2
        field_length = max(
            field_length,
            2 + len(field)
        )
        field_lengths.append(field_length)

    output_table_divider(field_lengths)
    # Column headings
    output_table_row(field_lengths, fields)
    output_table_divider(field_lengths)

    for entry in data:
        row = [
            entry[field] for field in fields
        ]
        output_table_row(field_lengths, row)
    output_table_divider(field_lengths)


def output_table_divider(lengths):
    output = '+'
    for length in lengths:
        output += '-' * length
        output += '+'
    print(output)


def output_table_row(lengths, entries):
    output = '|'
    for i in range(len(lengths)):
        output += str(entries[i]).center(lengths[i])
        output += '|'
    print(output)


def get_manager_version():
    global _MANAGER_VERSION
    if _MANAGER_VERSION:
        return _MANAGER_VERSION
    try:
        version_output = subprocess.check_output(['cfy', '--version'])
    except subprocess.CalledProcessError:
        return None

    edition = (
        'community' if 'Community edition' in version_output
        else 'premium'
    )

    for line in version_output.splitlines():
        if line.lower().startswith('cloudify manager'):
            # Expecting line to be similar to:
            # Cloudify Manager 4.2.0 [ip=10.239.3.199]
            version_string = line.split(' ')[2]
            break

    _MANAGER_VERSION = Version(version_string, edition=edition,
                               raw=version_output)

    return _MANAGER_VERSION


def load_patch_definition(filename):
    with open(filename) as definition_handle:
        definition = json.load(definition_handle)

    # Currently this handles version 1.0.0, 1.1.0, and 2.0.0 of patch
    # definitions:
    # {
    #   "patch_version": 1.0.0,  # or 1.1.0, 2.0.0, 2.1.0
    #   "manager_versions": [4.2],  # List of supported cloudify managers
    #   "community": <true|false>,  # Whether community is supported
    #   "premium": <true|false>,  # Whether premium is supported
    #   "patches": [  # List of patch files to apply.
    #                 # These should be in a format 'patch' on centos
    #                 # understands
    #     {
    #       "patch_file": "<patch file name in patch archive 'patches' dir>",
    #       "md5sum": "<md5sum of patch file>",  # This must be sha256sum for
    #                                              2.0.0 and above definitions
    #       "destinations": [  # List of locations that this file should patch
    #         "/path/to/location/on/manager",
    #         "/path/to/other/location/on/manager",
    #       ]
    #     },
    #     ... # More patch files, if any are required
    #   ],
    #   "affected_services": [  # List of services that will be stopped before
    #                           # applying the patches and started afterwards.
    #                           # These services will also be stopped before
    #                           # any rollbacks and started afterwards.
    #     "cloudify-service-1",
    #     "cloudify-service-2"
    #   ],
    #   # Node types are only valid for patch definition version 2.1.0+
    #   "node_types": [ # List of affected node types.
    #                   # Valid node typed are: manager, db, broker
    #                   # Node types other than manager are not valid for
    #                   # Cloudify versions below 5.0.5.
    #     "manager"
    #   ],
    #   # Note that for 2.0.0 and above, sha256sums are used instead of MD5s.
    #   # Therefore, these sections will be "sha256sums_before" and
    #   # "sha256sums_after", and will contain lists of SHA256sums instead of
    #   # MD5sums.
    #   "md5sums_before": {  # Each file will have md5sum checked before
    #                        # applying the patches. If a file on the
    #                        # manager is specified in this list and does not
    #                        # have an md5sum in the acceptable list for that
    #                        # file, the patch process will be aborted.
    #                        # Files in this list will be backed up before the
    #                        # patch is applied, and restored if it aborts for
    #                        # any reason after the initial checks.
    #     "/path/to/file1": ["<md5sum before patch is applied>"],
    #     "/path/to/file2": ["<md5sum before patch is applied>"],
    #     "/path/to/file3": ["<md5sum before patch is applied>",
    #                        "<other acceptable md5sum>"]
    #   },
    #   "md5sums_after": {  # After all commands have been run, these md5sums
    #                       # will be checked after all patches have been
    #                       # applied and will abort and roll back the patch.
    #                       # While the list doesn't have to be the same, it
    #                       # probably should be, usually.
    #     "/path/to/file1": ["<md5sum after patch is applied>"],
    #     "/path/to/file2": ["<md5sum after patch is applied>"],
    #     "/path/to/file3": ["<md5sum after patch is applied>",
    #                        "<other acceptable md5sum>"]
    #   }
    # }
    try:
        patch_version = Version(definition['patch_version'])
    except KeyError:
        add_definition_error(
            message=(
                'patch_version not found in definition. '
                'Please confirm patch is valid and not corrupted.'
            ),
            definition=definition,
            fatal=True,
        )
        return definition

    try:
        patch_schema = PATCH_VERSIONS[patch_version[0]]
        if patch_version[1] > 0:
            minor_schema = patch_schema['minor_versions'][patch_version[1]]
        else:
            minor_schema = {}
    except KeyError:
        supported_versions = get_supported_definition_versions()

        add_definition_error(
            message=(
                'Only definition versions {supported} are supported '
                'currently. Found version: {version}'.format(
                    supported=', '.join(supported_versions),
                    version='.'.join([str(i) for i in patch_version]),
                )
            ),
            definition=definition,
            fatal=True,
        )
        return definition

    required_keys = patch_schema.get('required', set())
    required_keys.update(minor_schema.get('required', set()))

    recommended_keys = patch_schema.get('recommended', set())
    recommended_keys.update(minor_schema.get('recommended', set()))

    # Unexpected keys might cause interesting behaviour. We'll avoid that.
    allowed_keys = required_keys | recommended_keys
    allowed_keys.add('errors')

    missing_keys = required_keys - set(definition)
    if missing_keys:
        add_definition_error(
            message=(
                'Patch definition must contain {keys}. '
                'Keys found: {found}'.format(
                    keys=', '.join(missing_keys),
                    found=', '.join(definition.keys()),
                )
            ),
            definition=definition,
            fatal=True,
        )

    missing_recommended = recommended_keys - set(definition)
    if missing_recommended:
        add_definition_error(
            message=(
                'Patch definition should contain {keys}. '
                'Keys found: {found}'.format(
                    keys=', '.join(missing_recommended),
                    found=', '.join(definition.keys()),
                )
            ),
            definition=definition,
        )

    unsupported_keys = set(definition) - allowed_keys
    if unsupported_keys:
        add_definition_error(
            message=(
                "Patch definition version {version} must not contain "
                "{keys}. Supported keys are: {supported}".format(
                    keys=', '.join(unsupported_keys),
                    version=definition['patch_version'],
                    supported=', '.join(allowed_keys),
                )
            ),
            definition=definition,
            fatal=True,
        )

    if definition.get('description'):
        # To make anything that displays the description with any other
        # information (such as display_patch) easier to structure.
        if not definition['description'].endswith('.'):
            add_definition_error(
                message='Patch description must end with a full stop.',
                definition=definition,
                fatal=True,
            )

    if definition.get('node_types') and 'node_types' not in unsupported_keys:
        if get_manager_version() >= Version('5.0.5'):
            supported_node_types = NODE_TYPES
        else:
            supported_node_types = ['manager']
        unsupported_node_types = [
            node_type
            for node_type in definition['node_types']
            if node_type not in supported_node_types
        ]
        if unsupported_node_types:
            add_definition_error(
                message=(
                    'Unsupported node types found in patch definition: '
                    '{unsupported}'.format(unsupported=', '.join(
                        unsupported_node_types,
                    ))
                ),
                definition=definition,
                fatal=True,
            )

    return definition


def load_and_validate_patch_definition(filename, action):
    definition = load_patch_definition(filename)

    fatal_error = False
    for error in definition.get('errors', []):
        message = error['message']
        if error['fatal']:
            message = 'FATAL: ' + message
            fatal_error = True
        sadly_say(message)
    if fatal_error:
        exit_with_sadness('Patch definition error!', action)

    return definition


def add_definition_error(message, definition, fatal=False):
    errors = definition.get('errors', [])
    errors.append({
        'message': message,
        'fatal': fatal,
    })
    definition['errors'] = errors


def get_unhealthy_services():
    if get_manager_version() >= Version('5.0.5'):
        command = ['cluster', 'status']
        expected = ['OK']
    else:
        command = ['status']
        expected = ['running', 'up']

    try:
        _, results = get_cfy_output(command)
    except subprocess.CalledProcessError:
        return ['All processes (is cfy pointing at a working manager?)']

    if len(results) == 0:
        return ['All processes (is cfy pointing at a working manager?)']

    unhealthy_services = []

    for result in results:
        service = result[1]
        status = result[2]
        if status not in expected:
            unhealthy_services.append(service)

    return unhealthy_services


def get_profile_ssh_details(action):
    try:
        headers, results = get_cfy_output(['profiles', 'show-current'])
    except subprocess.CalledProcessError as err:
        exit_with_sadness(
            (
                'Failed to retrieve SSH details from profile, with error: '
                '{err}'
            ).format(
                err=str(err),
            ),
            action,
        )

    ssh_user_pos = headers.index('ssh_user')
    ssh_port_pos = headers.index('ssh_port')
    ssh_key_path_pos = headers.index('ssh_key_path')
    try:
        if get_manager_version() >= Version('5.0.5'):
            cluster_node_name_pos = headers.index('Manager hostname')
            manager_ip_pos = headers.index('Manager ip')
        else:
            cluster_node_name_pos = headers.index('cluster node name')
            manager_ip_pos = headers.index('manager_ip')
    except ValueError:
        cluster_node_name_pos = None
        manager_ip_pos = headers.index('manager_ip')

    manager_connection_details = []
    for result in results:
        manager_connection_details.append({
            'manager_ip': result[manager_ip_pos],
            'ssh_user': result[ssh_user_pos],
            'connection_string': (
                result[ssh_user_pos] + '@' + result[manager_ip_pos]
            ),
            'ssh_port': result[ssh_port_pos],
            'key_path': result[ssh_key_path_pos],
            'cluster_node_name': (
                result[cluster_node_name_pos]
                if cluster_node_name_pos else None
            ),
        })

    return manager_connection_details


def _get_node_details(type_name, ssh_base, name_ref, address_ref):
    headers, results = get_cfy_output(['cluster', type_name, 'list'])

    name_pos = headers.index(name_ref)
    addr_pos = headers.index(address_ref)

    nodes = []

    for result in results:
        node_name = result[name_pos]
        node_address = result[addr_pos]
        node = copy.copy(ssh_base)
        node.update({
            'manager_ip': node_address,
            'connection_string': ssh_base['ssh_user'] + '@' + node_address,
            'cluster_node_name': node_name,
        })
        nodes.append(node)

    return nodes


def get_cluster_nodes(ssh_details, include_node_types=None):
    if include_node_types is None:
        include_node_types = ['manager']

    for element in include_node_types:
        if element not in NODE_TYPES:
            raise InvalidNodeType(
                'Node type {specified} is invalid. Valid node types are: '
                '{valid}'.format(
                    specified=element,
                    valid=', '.join(NODE_TYPES),
                )
            )

    all_in_one = len(ssh_details) == 1

    # Because we don't store separate ssh information for the brokers and dbs,
    # we need to assume that all the nodes use the same ssh details, just with
    # different addresses
    ssh_base = ssh_details[0]

    nodes = []

    if 'manager' in include_node_types:
        nodes.extend(
            _get_node_details('managers', ssh_base, 'hostname', 'private_ip')
        )
    if not all_in_one and 'db' in include_node_types:
        nodes.extend(
            _get_node_details('db-nodes', ssh_base, 'name', 'host')
        )
    if not all_in_one and 'broker' in include_node_types:
        nodes.extend(
            _get_node_details('brokers', ssh_base, 'name', 'host')
        )

    return nodes


def get_cluster_members():
    headers, results = get_cfy_output(['cluster', 'nodes', 'list'])

    state_pos = headers.index('state')
    name_pos = headers.index('name')

    leader_node = None
    member_nodes = []
    offline_nodes = []
    for result in results:
        node_name = result[name_pos]
        node_state = result[state_pos]
        if node_state == 'leader':
            if leader_node is not None:
                raise ClusterUnhealthy(
                    'Found multiple leader nodes in the cluster. This '
                    'should not happen! Both {leader1} and {leader2} had '
                    'their state set to "leader".'.format(
                        leader1=leader_node,
                        leader2=node_name,
                    ),
                )
            leader_node = node_name
        elif node_state == 'replica':
            member_nodes.append(node_name)
        elif node_state == 'offline':
            offline_nodes.append(node_name)
        else:
            raise ClusterUnhealthy(
                'Unexpected cluster node state found: '
                '{node} is in state {state}.'.format(
                    node=node_name,
                    state=node_state,
                ),
            )

    return leader_node, member_nodes, offline_nodes


def change_cluster_leader(new_leader):
    leader, _, _ = get_cluster_members()
    if leader == new_leader:
        raise ClusterLeaderChangeToCurrent()

    say("Changing cluster leader to {new}.".format(new=new_leader))
    subprocess.check_call(['cfy', 'cluster', 'set-active', new_leader])

    # It can take a minute or more for failovers to occur. We'll allow for
    # five minutes in case of excessively slow changes on a large manager
    max_attempts = 100
    for attempt in range(1, max_attempts + 1):
        leader, _, offline = get_cluster_members()
        if offline:
            say(
                "Waiting for cluster members to come back online "
                "[{at}/{mx}]...".format(at=attempt, mx=max_attempts)
            )
        elif leader == new_leader:
            say("Cluster leader is now {new}.".format(new=new_leader))
            break
        else:
            say(
                "Waiting for {new} to become leader "
                "(current leader: {current}) [{at}/{mx}]...".format(
                    new=new_leader,
                    current=leader,
                    at=attempt,
                    mx=max_attempts,
                )
            )
        time.sleep(3)

    if leader != new_leader:
        raise ClusterLeaderChangeFailure(
            'Cluster leader did not become {new} within the timeout '
            'period.'.format(new=new_leader),
        )


def get_cfy_output(subcommand_and_args):
    command = ['cfy']
    command.extend(subcommand_and_args)

    command_output = subprocess.check_output(command)

    headers = []
    results = []
    dividers_found = 0
    for line in command_output.splitlines():
        if dividers_found == 3:
            # We reached the end of the output
            break

        line = line.strip()

        # Expecting something like:
        # Getting management services status... [ip=192.0.2.4]
        #
        # Services:
        # +--------------------------------+---------+
        # |            service             |  status |
        # +--------------------------------+---------+
        # | Cloudify Composer              | running |
        # | AMQP-Postgres                  | running |
        # | RabbitMQ                       | running |
        # | Webserver                      | running |
        # | Management Worker              | running |
        # | PostgreSQL                     | running |
        # | Cloudify Console               | running |
        # | Manager Rest-Service           | running |
        # +--------------------------------+---------+
        if line.startswith('+--'):
            dividers_found += 1
        elif dividers_found == 1:
            # Headers are found after the first divider
            headers = [element.strip() for element in line.split('|')]
        elif dividers_found == 2:
            # Results are only shown after the second divider
            results.append([element.strip() for element in line.split('|')])

    return headers, results


def get_file_hash(file_path, manager_connection_string, ssh_port, keyfile,
                  hash_type):
    if hash_type == 'md5':
        command = 'md5sum'
    elif hash_type == 'sha256':
        command = '/opt/cloudify/pysha256sum'
    else:
        # This shouldn't be user-visible unless someone really messes up the
        # calling code
        raise RuntimeError('Hash type {hash_type} not supported.'.format(
            hash_type=hash_type,
        ))

    # Structure the command so that we return meaningful output in predictable
    # situations (where the path isn't a file (e.g. is a directory) or does
    # not exist), while still allowing failure for unexpected situations
    # (e.g. segfault on one of the commands)
    md5command = (
        'if sudo test -f {path} ; '
        # sudo this so that we can avoid permissions issues
        '  then sudo {command} {path}; '
        'elif sudo test -e {path}; '
        '  then echo -n "{notafile}"; '
        'else '
        '  echo -n "{notexisting}"; '
        'fi'.format(
            command=command,
            path=file_path,
            notafile="NOTAFILE",
            notexisting="DOESNOTEXIST",
        )
    )
    return ssh(
        manager_connection_string, ssh_port, keyfile, md5command
    ).split(' ')[0]


def get_local_hash(file_path, hash_type):
    if hash_type == 'md5':
        with open(file_path) as data_handle:
            return hashlib.md5(data_handle.read()).hexdigest()
    elif hash_type == 'sha256':
        with open(file_path) as data_handle:
            return hashlib.sha256(data_handle.read()).hexdigest()
    else:
        # This shouldn't be user-visible unless someone really messes up the
        # calling code
        raise RuntimeError('Hash type {hash_type} not supported.'.format(
            hash_type=hash_type,
        ))


def upload_hash_commands(manager_connection_string, ssh_port, keyfile):
    target_folder = '/opt/cloudify'
    # Don't break if this script is run on windows (os.path.join will
    # cause problems in that case)
    script_path = target_folder + '/pysha256sum'

    say('Checking hash commands on server.')
    expected_hash = hashlib.md5(SHA256SUM_SCRIPT).hexdigest()
    server_hash = get_file_hash(
        script_path, manager_connection_string, ssh_port, keyfile,
        # Using md5 because if the script isn't there then calling it will be
        # an error
        hash_type='md5',
    )

    if expected_hash == server_hash:
        say('Hash check script on server is up to date.')
        return

    if server_hash != 'DOESNOTEXIST':
        say('Removing outdated hash check script.')
        ssh(
            manager_connection_string, ssh_port, keyfile,
            'sudo rm -rf ' + script_path,
        )

    say("Uploading hash check script.")
    with tempdir() as tmp_path:
        with open(
            os.path.join(tmp_path, 'pysha256sum'),
            'w',
        ) as script_handle:
            script_handle.write(SHA256SUM_SCRIPT)

        upload(
            'pysha256sum',
            tmp_path,
            script_path,
            manager_connection_string,
            ssh_port,
            keyfile,
            # Using md5 because otherwise we'll be trying to validate that
            # the script uploaded without corruption using the possibly
            # corrupt script
            hash_type='md5',
        )

        ssh(
            manager_connection_string, ssh_port, keyfile,
            'sudo chown root. {target}'.format(
                target=script_path,
            )
        )
        ssh(
            manager_connection_string, ssh_port, keyfile,
            'sudo chmod 555 {target}'.format(
                target=script_path,
            )
        )


def hash_mismatches(check_list, manager_connection_string, ssh_port, keyfile,
                    hash_type):
    mismatches = []
    for file_path, acceptable_hashes in check_list.items():
        say(
            'Checking {hash_type}sum for {path}'.format(
                path=file_path,
                hash_type=hash_type,
            )
        )
        file_hash = get_file_hash(
            file_path, manager_connection_string, ssh_port, keyfile,
            hash_type,
        )

        if file_hash in acceptable_hashes:
            # This file is fine, don't complain
            continue

        # Hash mismatch, give helpful error messages
        mismatches.append(file_path)
        if acceptable_hashes == ["DOESNOTEXIST"]:
            message = (
                '{file_path} should not exist, but file with '
                '{hash_type}sum {file_hash} was found.'
            )
        else:
            message = (
                '{file_path} has an incorrect {hash_type}sum. '
                '{hash_type}sum was {file_hash}. '
                'Allowed {hash_type}sums are {acceptable}'
            )

        sadly_say(message.format(
            file_path=file_path,
            file_hash=file_hash,
            acceptable=', '.join(acceptable_hashes),
            hash_type=hash_type,
        ))

    return mismatches


def can_ssh(manager_connection_string, ssh_port, keyfile):
    try:
        ssh(manager_connection_string, ssh_port, keyfile, 'echo "SSH works."',
            stderr=subprocess.STDOUT)
        return True
    except subprocess.CalledProcessError as err:
        if err.returncode == 255 and 'usage' in err.output:
            sadly_say(
                "Invalid arguments to SSH command. Command was: {cmd}".format(
                    cmd=' '.join(err.cmd),
                )
            )
        else:
            sadly_say(
                'SSH attempt failed due to error:\n{err}\nCommand was: '
                '{cmd}'.format(
                    err=err.output,
                    cmd=' '.join(err.cmd),
                )
            )
        return False


def patch_command_available(manager_connection_string, ssh_port, keyfile):
    try:
        ssh(manager_connection_string, ssh_port, keyfile, 'which patch')
        return True
    except subprocess.CalledProcessError:
        return False


def ssh(manager_connection_string, ssh_port, keyfile, command,
        stderr=None):
    ssh_command = ['ssh', '-p', ssh_port]
    if keyfile:
        ssh_command.extend(['-i', keyfile])
    ssh_command.extend([manager_connection_string, command])
    return subprocess.check_output(ssh_command, stderr=stderr)


@contextmanager
def tempdir():
    tmp_path = tempfile.mkdtemp(prefix='patchify-')
    try:
        yield tmp_path
    finally:
        shutil.rmtree(tmp_path)


def _scp(keyfile, ssh_port, source, dest, quiet=False):
    scp_command = ['scp', '-P', ssh_port]
    if keyfile:
        scp_command.extend(['-i', keyfile])
    scp_command.extend([source, dest])
    if quiet:
        subprocess.check_output(scp_command)
    else:
        subprocess.check_call(scp_command)


def upload(file_name, source_path, dest_path,
           manager_connection_string, ssh_port, keyfile,
           hash_type='sha256', expected_hash=None):
    source = os.path.join(source_path, file_name)
    tmp_path = '/tmp/' + file_name
    dest = '{mgr}:{dest}'.format(
        mgr=manager_connection_string,
        dest=tmp_path,
    )
    say("Uploading {source} to {dest}".format(source=source, dest=dest))

    _scp(keyfile, ssh_port, source, dest)

    if not expected_hash:
        expected_hash = get_local_hash(source, hash_type=hash_type)

    uploaded_hash = get_file_hash(
        file_path=tmp_path,
        manager_connection_string=manager_connection_string,
        ssh_port=ssh_port,
        keyfile=keyfile,
        hash_type=hash_type,
    )

    if uploaded_hash != expected_hash:
        raise UploadError(
            'Uploaded file {file_name} had hash {actual}, but should '
            'have had {expected}.'.format(
                file_name=file_name,
                actual=uploaded_hash,
                expected=expected_hash,
            )
        )

    ssh(
        manager_connection_string,
        ssh_port,
        keyfile,
        'sudo mv {tmp_path} {dest_path}'.format(
            tmp_path=tmp_path,
            dest_path=dest_path,
        ),
    )

    ssh(
        manager_connection_string,
        ssh_port,
        keyfile,
        'sudo chown root. {dest_path}'.format(
            dest_path=dest_path,
        ),
    )


def download(file_name, source_path, dest_path,
             manager_connection_string, ssh_port, keyfile, quiet=False):
    tmp_path = '/tmp/' + file_name
    src_path = source_path + '/' + file_name
    source = '{mgr}:{source}'.format(
        mgr=manager_connection_string,
        source=tmp_path,
    )
    dest = dest_path
    if not quiet:
        say("Downloading {source} to {dest}".format(source=src_path,
                                                    dest=dest))

    ssh(
        manager_connection_string,
        ssh_port,
        keyfile,
        'sudo cp {src_path} {tmp_path} '
        '&& sudo chown $(whoami). {tmp_path} '
        '&& sudo chmod 400 {tmp_path}'.format(
            src_path=src_path,
            tmp_path=tmp_path,
        ),
    )

    _scp(keyfile, ssh_port, source, dest, quiet=quiet)

    original_md5sum = get_file_hash(
        file_path=os.path.join(source_path, file_name),
        manager_connection_string=manager_connection_string,
        ssh_port=ssh_port,
        keyfile=keyfile,
        hash_type='md5',
    )

    downloaded_md5sum = get_local_hash(dest, hash_type='md5')

    ssh(
        manager_connection_string,
        ssh_port,
        keyfile,
        'sudo rm {tmp_path}'.format(
            tmp_path=tmp_path,
        ),
    )

    if not original_md5sum == downloaded_md5sum:
        raise DownloadError(
            'Downloaded file {file_path} had md5sum {actual}, but should '
            'have had {expected}.'.format(
                file_path=os.path.join(dest_path, file_name),
                actual=downloaded_md5sum,
                expected=original_md5sum,
            )
        )


@contextmanager
def ignore_optional_service(service):
    """Ignore failure if service is optional.

    Not all services must exist on the manager: cloudify-handler-runner,
    cloudify-check-runner, cloudify-consul-watcher, and cloudify-syncthing
    will only exist after a cluster has been created
    """
    try:
        yield
    except subprocess.CalledProcessError as e:
        # 5 = EXIT_NOTINSTALLED = The program is not installed.
        if e.returncode == 5 and service in OPTIONAL_SERVICES:
            pass
        else:
            raise


def stop_services(manager_connection_string, ssh_port, services, keyfile):
    for service in services:
        with ignore_optional_service(service):
            ssh(
                manager_connection_string,
                ssh_port,
                keyfile,
                'sudo systemctl stop {service}'.format(service=service),
            )


def start_services(manager_connection_string, ssh_port, services, keyfile):
    # Ensure we're using any unit files we may have updated
    ssh(
        manager_connection_string,
        ssh_port,
        keyfile,
        'sudo systemctl daemon-reload',
    )
    for service in services:
        with ignore_optional_service(service):
            ssh(
                manager_connection_string,
                ssh_port,
                keyfile,
                'sudo systemctl start {service}'.format(service=service),
            )


def upload_and_verify_patch_files(manager_connection_string, ssh_port,
                                  patch_dir, patches, staging_dir,
                                  keyfile):
    for patch in patches:
        patch_name = patch['patch_file']
        if 'sha256sum' in patch:
            hash_type = 'sha256'
            expected_hash = patch['sha256sum']
        else:
            hash_type = 'md5'
            expected_hash = patch['md5sum']

        upload(
            file_name=patch_name,
            source_path=patch_dir,
            dest_path=os.path.join(staging_dir, patch_name),
            expected_hash=expected_hash,
            hash_type=hash_type,
            manager_connection_string=manager_connection_string,
            ssh_port=ssh_port,
            keyfile=keyfile,
        )


def build_manager_version_output_string(version, community):
    version_output = version
    if community:
        version_output += ' community'
    else:
        version_output += ' premium'
    return version_output


def rollback(manager_connection_string, ssh_port, patch_definition,
             backup_root, delete_root, keyfile, skip_services):
    if skip_services:
        sadly_say(
            "Not ensuring services are stopped due to skip-services flag."
        )
    else:
        # Make sure all services are stopped, which might not be the case
        # depending on when we had to begin the rollback.
        sadly_say('Ensuring services are stopped for rollback.')
        stop_services(
            manager_connection_string,
            ssh_port,
            patch_definition['affected_services'],
            keyfile,
        )

    sadly_say('Rolling back file changes.')
    affected_files = get_list_of_affected_files(patch_definition['patches'])
    for affected_file in affected_files:
        backed_up_file = os.path.join(
            backup_root,
            affected_file.lstrip('/'),
        )
        delete_dest_parent = os.path.join(
            delete_root,
            os.path.split(affected_file.lstrip('/'))[0]
        )
        delete_dest = os.path.join(
            delete_root,
            affected_file.lstrip('/')
        )
        ssh(
            manager_connection_string,
            ssh_port,
            keyfile,
            'if sudo test -f {backup}; then '    # Only restore existing
            '  sudo cp {backup} {orig}; '        # and remove added files
            'else '
            '  sudo mkdir -p {delete_dest_parent} && '
            '  sudo mv {orig} {delete_dest}; '
            'fi'.format(
                backup=backed_up_file,
                orig=affected_file,
                delete_dest_parent=delete_dest_parent,
                delete_dest=delete_dest,
            ),
        )

    if skip_services:
        sadly_say("Not restarting services due to skip-services flag.")
    else:
        sadly_say('Starting services after rollback.')
        start_services(
            manager_connection_string,
            ssh_port,
            patch_definition['affected_services'],
            keyfile,
        )

    # TODO: We should check service state here

    sadly_say('Rollback complete.')


def get_list_of_affected_files(patches):
    affected_files = []
    for patch in patches:
        affected_files.extend(patch['destinations'])
    return set(affected_files)


def get_patch_storage_path(timestamp):
    # Don't use os.path.join in case this gets run on windows (the path itself
    # is on the manager, which will be Linux)
    return '{base}/{timestamp}'.format(
        base=BASE_MANAGER_PATCH_STORAGE_PATH,
        timestamp=timestamp,
    )


def abort_on_ssh_failure(manager_connection_string, ssh_port, keyfile,
                         cluster_node_name=None, quiet=False):
    if not quiet:
        say("Checking SSH.")
    if not can_ssh(manager_connection_string, ssh_port, keyfile):
        exit_message = (
            'Failed to SSH into manager with settings:\n'
            'Connection string (user@manager_ip): {conn_string}\n'
            'Port: {port}\n'
            'Keyfile: {key}\n'
            'Manager ssh profiles settings may be incorrect.\n'
            'To view current profile settings, run: '
            'cfy profiles show-current\n'
            'Then ensure that ssh_user is correct.\n'
        ).format(
            conn_string=manager_connection_string,
            port=ssh_port,
            key=keyfile or '<SSH default>',
        )
        if cluster_node_name:
            exit_message += (
                'cfy profiles set-cluster {node} --ssh-user '
                '<SSH username>\n'.format(node=cluster_node_name)
            )
        else:
            exit_message += (
                'cfy profiles set --ssh-user <SSH username>\n'
            )
        exit_message += (
            'If your ssh port and keyfiles are not default, you should '
            'also correct these settings:\n'
        )
        if cluster_node_name:
            exit_message += (
                'cfy profiles set-cluster {node} --ssh-key <SSH key path>\n'
                'cfy profiles set-cluster {node} --ssh-port '
                '<SSH port>'.format(node=cluster_node_name)
            )
        else:
            exit_message += (
                'cfy profiles set --ssh-key <SSH key path>\n'
                'cfy profiles set --ssh-port <SSH port>'
            )
        raise SSHCheckFailure(exit_message)


def abort_on_unhealthy_services():
    say("Checking service state.")
    unhealthy_services = get_unhealthy_services()
    if unhealthy_services:
        raise ServiceHealthFailure(
            'Not all services are healthy. '
            'Unhealthy services: {services}'.format(
                services=', '.join(unhealthy_services),
            ),
        )


def abort_on_missing_patch_command(manager_connection_string, ssh_port,
                                   keyfile, action, install_patch_command):
    say('Confirming patch command exists on server.')
    if not patch_command_available(manager_connection_string, ssh_port,
                                   keyfile):
        if install_patch_command:
            say("Patch command not found. Installing patch command.")
            ssh(
                manager_connection_string,
                ssh_port,
                keyfile,
                'sudo yum install -y patch',
            )
        else:
            exit_with_sadness(
                'Could not find "patch" command on server. '
                'On the manager, you may need to run: '
                '"sudo yum install -y patch" or call this script with '
                'the --install-patch-command flag.',
                action,
            )


def check_hashes_before(patch_definition, manager_connection_string,
                        ssh_port, keyfile, hash_type):
    definition_key = hash_type + 'sums_before'
    after_key = hash_type + 'sums_after'

    say("Checking hashes are as expected.")
    mismatches = hash_mismatches(patch_definition[definition_key],
                                 manager_connection_string,
                                 ssh_port,
                                 keyfile,
                                 hash_type=hash_type)
    if mismatches:
        already_applied = {
            file_path: hashes
            for file_path, hashes in patch_definition[after_key].items()
            if file_path in mismatches
        }
        unexpected = hash_mismatches(already_applied,
                                     manager_connection_string,
                                     ssh_port,
                                     keyfile,
                                     hash_type=hash_type)
        if unexpected:
            raise HashCheckFailure(
                'Unexpected hashes found before starting patch installation.',
            )
        else:
            # List of files that are already patched (thanks, syncthing)
            return already_applied
    else:
        return {}


def generate_patch_data_dir(patch_name, patch_definition, timestamp,
                            manager_connection_string, ssh_port, keyfile,
                            definition_file, action):
    patch_dirs = {}

    patch_path = get_patch_storage_path(timestamp)
    patch_dirs['patch_path'] = patch_path

    say("Creating patch directory in: {path}".format(path=patch_path))
    # We will also put some patch details to aid any later troubleshooting
    patch_info = 'attempting to apply ' + patch_name
    ssh(
        manager_connection_string,
        ssh_port,
        keyfile,
        'sudo mkdir -p ' + patch_path +
        ' && echo ' + patch_info +
        ' | sudo tee ' + patch_path + '/patch_info >/dev/null',
    )
    source_path, file_name = os.path.split(definition_file)
    upload(
        file_name=file_name,
        source_path=source_path,
        dest_path=os.path.join(patch_path, 'patch_definition'),
        manager_connection_string=manager_connection_string,
        ssh_port=ssh_port,
        keyfile=keyfile,
    )

    patch_dirs['backup_root'] = '{base}/backups'.format(
        base=patch_path,
    )
    patch_dirs['delete_root'] = '{base}/deleted_files'.format(
        base=patch_path,
    )
    say(
        "Backing up affected files to {path}".format(
            path=patch_dirs['backup_root']
        )
    )
    ssh(
        manager_connection_string,
        ssh_port,
        keyfile,
        'sudo mkdir -p ' + patch_dirs['backup_root'],
    )
    ssh(
        manager_connection_string,
        ssh_port,
        keyfile,
        'sudo mkdir -p ' + patch_dirs['delete_root'],
    )
    affected_files = get_list_of_affected_files(patch_definition['patches'])
    for affected_file in affected_files:
        say(
            "Backing up {file_path} to {destination}".format(
                file_path=affected_file,
                destination=os.path.join(
                    patch_dirs['backup_root'],
                    affected_file.lstrip('/'),
                ),
            )
        )
        # We sudo (root) the copy because of user differences in different
        # versions of cloudify, but we probably should define this better
        # based on which version it's targetting.
        ssh(
            manager_connection_string,
            ssh_port,
            keyfile,
            'if sudo test -f {source}; then '  # Only if the file exists
            'sudo cp --parents {source} {destination}; fi'.format(
                source=affected_file,
                destination=patch_dirs['backup_root'],
            ),
        )

    if action == 'APPLYING PATCH':
        patch_dirs['staging_dir'] = '{base}/patch_files'.format(
            base=patch_path,
        )
        ssh(
            manager_connection_string,
            ssh_port,
            keyfile,
            'sudo mkdir -p ' + patch_dirs['staging_dir'],
        )
        patch_dir = os.path.join(os.path.dirname(definition_file),
                                 'patches')
        say("Uploading and verifying patch files.")
        try:
            upload_and_verify_patch_files(manager_connection_string,
                                          ssh_port,
                                          patch_dir,
                                          patch_definition['patches'],
                                          patch_dirs['staging_dir'],
                                          keyfile)
        except UploadError as err:
            raise UploadError(
                'Failed to upload patch files: {err}'.format(
                    err=str(err),
                ),
            )

    return patch_dirs


def generate_timestamp():
    # Avoiding : in timestamp to avoid needing to escape it for bash
    return datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S%z')


def possibly_stop_services(patch_definition, skip_services,
                           manager_connection_string, ssh_port, keyfile):
    if skip_services:
        say("Not stopping services due to skip-services flag.")
    else:
        say("Stopping services.")
        stop_services(manager_connection_string,
                      ssh_port,
                      patch_definition['affected_services'],
                      keyfile)


def possibly_start_services(patch_definition, skip_services,
                            manager_connection_string, ssh_port, keyfile):
    if skip_services:
        say("Not starting services due to skip-services flag.")
    else:
        say("Starting services.")
        start_services(manager_connection_string,
                       ssh_port,
                       patch_definition['affected_services'],
                       keyfile)


def check_hashes_after(patch_definition,
                       manager_connection_string, ssh_port, keyfile,
                       backup_root, delete_root, skip_services,
                       hash_type):
    definition_key = hash_type + 'sums_after'

    say("Checking post-patch hashes.")
    if hash_mismatches(patch_definition[definition_key],
                       manager_connection_string,
                       ssh_port,
                       keyfile,
                       hash_type):
        rollback(manager_connection_string, ssh_port, patch_definition,
                 backup_root, delete_root, keyfile, skip_services)
        raise HashCheckFailure(
            'Unexpected hashes found after patch installation.\n'
            'Changes rolled back.',
        )


def wait_for_healthy_services(patch_definition, backup_root, delete_root,
                              skip_services,
                              manager_connection_string, ssh_port, keyfile):
    say("Checking service state...")
    unhealthy_services = []
    max_attempts = 20
    for attempt in range(1, max_attempts + 1):
        if unhealthy_services:
            say(
                "Services still restarting [{at}/{mx}]...".format(
                    at=attempt,
                    mx=max_attempts,
                )
            )
            time.sleep(3)
        if attempt == max_attempts // 2:
            if skip_services:
                continue
            say("Services not yet restarted, attempting start again in case "
                "of unresolved dependency issues.")
            possibly_start_services(patch_definition, skip_services,
                                    manager_connection_string, ssh_port,
                                    keyfile)
        unhealthy_services = get_unhealthy_services()
    if unhealthy_services:
        rollback(manager_connection_string, ssh_port, patch_definition,
                 backup_root, delete_root, keyfile, skip_services)
        raise ServiceHealthFailure(
            'Services did not restart correctly. '
            'Unhealthy services: {services}\n'
            'Changes rolled back.'.format(
                services=', '.join(unhealthy_services),
            ),
        )


def patch_applier(definition_file, skip_services, install_patch_command,
                  skip_version_check):
    action = 'APPLYING PATCH'

    ssh_details = get_profile_ssh_details(action)

    say("Loading patch definition.")
    patch_definition = load_and_validate_patch_definition(definition_file,
                                                          action)

    patch_name = os.path.basename(definition_file)
    if patch_name.endswith('.json'):
        # Remove patch name extension
        patch_name = os.path.splitext(patch_name)[0]

    if not skip_version_check:
        say("Checking manager version is valid.")
        manager_version = get_manager_version()
        if manager_version is None:
            exit_with_sadness(
                'Could not get manager version! '
                'Are you in an environment with cfy using a profile for the '
                'manager you intend to patch?',
                action,
            )
        supported_versions = []
        if patch_definition['premium']:
            for version in patch_definition['manager_versions']:
                supported_versions.append(
                    Version(version, edition='premium')
                )
        if patch_definition['community']:
            for version in patch_definition['manager_versions']:
                supported_versions.append(
                    Version(version, edition='community')
                )
        if manager_version not in supported_versions:
            try:
                manager_version_output = build_manager_version_output_string(
                    version=manager_version,
                    community=manager_version.edition == 'community',
                )
            except Exception as err:
                exit_with_sadness(
                    'Could not determine manager version.\n'
                    'Version check output was: {raw_output}\n'
                    'Error was: {err}'.format(
                        raw_output=manager_version.raw,
                        err=str(err),
                    ),
                    action,
                )
            supported_versions_output = ', '.join([
                build_manager_version_output_string(
                    version=version,
                    community=version.edition == 'community',
                )
                for version in supported_versions
            ])
            exit_with_sadness(
                'Manager version mismatch. '
                'Manager is {manager_version}, but patch only supports '
                '{supported_versions}'.format(
                    manager_version=manager_version_output,
                    supported_versions=supported_versions_output,
                ),
                action,
            )

    if len(ssh_details) > 1:
        say("Attempting to apply patch to cluster.")

        if get_manager_version() >= Version('5.0.5'):
            ssh_details = get_cluster_nodes(
                ssh_details,
                include_node_types=patch_definition.get('node_types'),
            )

            members = [entry['cluster_node_name'] for entry in ssh_details]
            # There are no leaders in 5.0.5
            leader = None
        else:
            leader, members, offline = get_cluster_members()

            if offline:
                exit_with_sadness(
                    (
                        'Cluster members {offline} are offline. Please ensure '
                        'cluster is healthy before attempting to apply the '
                        'patch.'.format(
                            offline=', '.join(offline),
                        )
                    ),
                    action,
                )

            # We will process the members in arbitrary order, followed by the
            # current leader, as we need to switch the leader after each
            # application to ensure services still work without the patch
            members.append(leader)

        member_ssh = {
            entry['cluster_node_name']: entry for entry in ssh_details
        }
        say("Validating SSH connectivity to members.")
        for member in members:
            try:
                abort_on_ssh_failure(
                    member_ssh[member]['connection_string'],
                    member_ssh[member]['ssh_port'],
                    member_ssh[member]['key_path'],
                    cluster_node_name=member,
                )
                abort_on_missing_patch_command(
                    member_ssh[member]['connection_string'],
                    member_ssh[member]['ssh_port'],
                    member_ssh[member]['key_path'],
                    action,
                    install_patch_command,
                )
            except SSHCheckFailure as err:
                exit_with_sadness(str(err), action)

        apply_not_required = 0

        for member in members:
            applied_to_this_node = True
            say(
                "Attempting to apply patch to {member}".format(
                    member=member,
                )
            )
            try:
                # We shouldn't play with services prior to 5.0.5 or we will
                # cause problems with the cluster
                skip_service_restart = (
                    get_manager_version() < Version('5.0.5')
                )
                rollback_args = apply_patch(
                    patch_definition,
                    patch_name,
                    definition_file,
                    skip_service_restart,
                    member_ssh[member],
                    action,
                )
            except (
                ServiceHealthFailure,
                PatchAlreadyApplied,
                PatchApplyFailed,
                PatchDefinitionParseFailure,
                HashCheckFailure,
                RegistryGenerationFailure,
            ) as err:
                if isinstance(err, PatchAlreadyApplied):
                    say(
                        "Patch did not need applying to {member}.".format(
                            member=member,
                        )
                    )
                    applied_to_this_node = False
                    apply_not_required += 1
                else:
                    exit_with_sadness(
                        (
                            'Failed to apply patch to node.\n'
                            'Aborting patch application attempt.'
                        ),
                        action,
                    )
            # Confirming health of cluster member
            try:
                if applied_to_this_node:
                    if get_manager_version() >= Version('5.0.5'):
                        # If the node came up but made the cluster unhealthy,
                        # this will raise a ServiceHealthFailure and we will
                        # roll back to avoid breaking anything further.
                        abort_on_unhealthy_services()
                    else:
                        # If no other nodes needed modifying, the leader will
                        # still be the original leader, so we must switch away
                        # then back
                        if member == leader \
                           and apply_not_required == len(members) - 1:
                            say(
                                "Switching away from old leader temporarily "
                                "to confirm services are healthy after "
                                "modification."
                            )
                            change_cluster_leader(members[0])
                        change_cluster_leader(member)
            except ServiceHealthFailure:
                sadly_say(
                    "After patching {member}, the cluster became unhealthy.\n"
                    "Attempting to roll back patch on this manager.".format(
                        member=member,
                    )
                )
                rollback(*rollback_args)
                exit_with_sadness(
                    (
                        "Rolled back {member}. Patch has not been fully "
                        "applied to cluster.\n"
                        "Please verify cluster healthy before re-attempting "
                        "this action."
                    ),
                    action,
                )
            except ClusterLeaderChangeFailure as err:
                sadly_say(
                    "Failed to switch to {member}.\n"
                    "Attempting to roll back patch on this manager.".format(
                        member=member,
                    )
                )
                rollback(*rollback_args)
                exit_with_sadness(
                    (
                        "Rolled back {member}. Patch has not been fully "
                        "applied to cluster.\n"
                        "Please verify cluster healthy before re-attempting "
                        "this action."
                    ),
                    action,
                )
            except ClusterLeaderChangeToCurrent as err:
                exit_with_sadness(
                    (
                        "Failure attempting to change leader to {member}, "
                        "because this node was already the leader.\n"
                        "Cluster appears to be undergoing unexpected leader "
                        "changes while patching.".format(
                            member=member,
                        )
                    ),
                    action,
                )
        if apply_not_required == len(members):
            exit_with_sadness(
                'Patch was already installed on all nodes in the cluster!',
                action,
            )
        else:
            say("Patch applied to cluster.")
    else:
        if get_manager_version() >= Version('5.0.5'):
            applies_to = patch_definition.get('node_types', [])
            if 'manager' not in applies_to:
                say("Patch does not apply to managers, skipping.")
                return
        say("Attempting to apply patch to manager.")
        ssh_details = ssh_details[0]
        try:
            abort_on_unhealthy_services()
            abort_on_ssh_failure(
                ssh_details['connection_string'],
                ssh_details['ssh_port'],
                ssh_details['key_path'],
            )
            abort_on_missing_patch_command(
                ssh_details['connection_string'],
                ssh_details['ssh_port'],
                ssh_details['key_path'],
                action,
                install_patch_command,
            )
            apply_patch(
                patch_definition,
                patch_name,
                definition_file,
                skip_services,
                ssh_details,
                action,
            )
        except (
            ServiceHealthFailure,
            SSHCheckFailure,
            PatchApplyFailed,
            PatchAlreadyApplied,
            PatchDefinitionParseFailure,
            HashCheckFailure,
            RegistryGenerationFailure,
        ) as err:
            exit_with_sadness(str(err), action)


def apply_patch(patch_definition, patch_name, definition_file, skip_services,
                ssh_details, action):
    manager_connection_string = ssh_details['connection_string']
    keyfile = ssh_details['key_path']
    ssh_port = ssh_details['ssh_port']

    # Use the correct hash type for this patch definition version
    if 'sha256sums_before' in patch_definition:
        hash_type = 'sha256'
    else:
        hash_type = 'md5'

    say("Retrieving patch registry information.")
    registry = download_patch_registry(manager_connection_string,
                                       ssh_port, keyfile)

    # SHA256 script is always needed, so make sure it is present
    upload_hash_commands(manager_connection_string, ssh_port, keyfile)

    say("Checking whether patch has already been applied.")
    definition_hash = get_local_hash(definition_file, hash_type='sha256')
    patch_id = generate_patch_id(patch_name, definition_hash)
    current_patches = registry.get('current_patches')
    this_patch = None
    for patch in current_patches:
        if patch['patch_id'] == patch_id:
            say('New patch ID located.')
            this_patch = patch
            break
    if this_patch is not None:
        raise PatchAlreadyApplied(
            'Existing patch found with ID {patch_id}.\n'
            'This patch appears to already be installed.'.format(
                patch_id=patch_id,
            )
        )

    skip_files = check_hashes_before(patch_definition,
                                     manager_connection_string,
                                     ssh_port, keyfile, hash_type=hash_type)

    timestamp = generate_timestamp()
    say("Patching timestamp is: {timestamp}".format(timestamp=timestamp))
    patch_dirs = generate_patch_data_dir(patch_name, patch_definition,
                                         timestamp,
                                         manager_connection_string, ssh_port,
                                         keyfile, definition_file, action)
    backup_root = patch_dirs['backup_root']
    delete_root = patch_dirs['delete_root']
    staging_dir = patch_dirs['staging_dir']

    possibly_stop_services(patch_definition, skip_services,
                           manager_connection_string, ssh_port, keyfile)

    say("Applying patches.")
    for patch in patch_definition['patches']:
        for destination in patch['destinations']:
            if destination in skip_files:
                # Cope with syncthing
                say(
                    'File {target} already patched, skipping.'.format(
                        target=destination,
                    )
                )
                continue
            say(
                'Applying {patch} to {target}'.format(
                    patch=patch['patch_file'],
                    target=destination,
                )
            )
            try:
                ssh(
                    manager_connection_string,
                    ssh_port,
                    keyfile,
                    'sudo patch {destination} {patch}'.format(
                        destination=destination,
                        patch=os.path.join(
                            staging_dir,
                            patch['patch_file'],
                        ),
                    )
                )
            except subprocess.CalledProcessError as err:
                rollback(manager_connection_string, ssh_port,
                         patch_definition, backup_root, delete_root, keyfile,
                         skip_services)
                raise PatchApplyFailed(
                    'Failed to apply patch {patch} to {target}\n'
                    'Changes rolled back.\n'
                    'Error was: {err}\n'.format(
                        patch=patch['patch_file'],
                        target=destination,
                        err=str(err),
                    ),
                )

    check_hashes_after(patch_definition,
                       manager_connection_string, ssh_port, keyfile,
                       backup_root, delete_root, skip_services,
                       hash_type=hash_type)

    possibly_start_services(patch_definition, skip_services,
                            manager_connection_string, ssh_port, keyfile)

    wait_for_healthy_services(patch_definition, backup_root, delete_root,
                              skip_services, manager_connection_string,
                              ssh_port, keyfile)

    say("Patch installed and services healthy.")

    say("Updating patch information and registry.")
    patch_info = "applied " + patch_name
    ssh(
        manager_connection_string,
        ssh_port,
        keyfile,
        'echo ' + patch_info +
        ' | sudo tee ' + patch_dirs['patch_path'] + '/patch_info >/dev/null',
    )
    add_patch_to_registry(registry, patch_id, timestamp, patch_name,
                          patch_dirs['patch_path'], patch_definition,
                          manager_connection_string, ssh_port, keyfile)

    say('Patch applied!')

    # Return arguments required for rollback to allow rollback in cluster
    # patching
    return (
        manager_connection_string,
        ssh_port,
        patch_definition,
        backup_root,
        delete_root,
        keyfile,
        skip_services,
    )


def patch_remover(target_patch_id, skip_services):
    action = 'REMOVING PATCH'

    ssh_details = get_profile_ssh_details(action)

    if len(ssh_details) > 1:
        say("Attempting to remove patch from cluster.")
        if get_manager_version() >= Version('5.0.5'):
            ssh_details = get_cluster_nodes(
                ssh_details,
                include_node_types=NODE_TYPES,
            )

            members = [entry['cluster_node_name'] for entry in ssh_details]
            # There are no leaders in 5.0.5
            leader = None
        else:
            leader, members, offline = get_cluster_members()

            if offline:
                exit_with_sadness(
                    (
                        'Cluster members {offline} are offline. Please '
                        'ensure cluster is healthy before attempting to '
                        'remove the patch.'.format(
                            offline=', '.join(offline),
                        )
                    ),
                    action,
                )

            # We will process the members in arbitrary order, followed by the
            # current leader, as we need to switch the leader after each
            # removal to ensure services still work without the patch
            members.append(leader)

        member_ssh = {
            entry['cluster_node_name']: entry for entry in ssh_details
        }

        say("Validating SSH connectivity to members.")
        for member in members:
            try:
                abort_on_ssh_failure(
                    member_ssh[member]['connection_string'],
                    member_ssh[member]['ssh_port'],
                    member_ssh[member]['key_path'],
                    cluster_node_name=member,
                )
            except SSHCheckFailure as err:
                exit_with_sadness(str(err), action)

        removal_not_required = 0

        for member in members:
            removed_from_this_node = True
            say(
                "Attempting to remove patch from {member}".format(
                    member=member,
                )
            )
            try:
                # We shouldn't play with services prior to 5.0.5 or we will
                # cause problems with the cluster
                skip_service_restart = (
                    get_manager_version() < Version('5.0.5')
                )
                rollback_args = remove_patch(
                    target_patch_id,
                    skip_service_restart,
                    member_ssh[member],
                    action,
                )
            except (
                ServiceHealthFailure,
                SSHCheckFailure,
                PatchNotFound,
                PatchRemovalBlocked,
                PatchDefinitionParseFailure,
                InvalidBackup,
                HashCheckFailure,
                RegistryGenerationFailure,
            ) as err:
                if isinstance(err, PatchNotFound):
                    say(
                        "Patch did not need removing from {member}.".format(
                            member=member,
                        )
                    )
                    removed_from_this_node = False
                    removal_not_required += 1
                else:
                    exit_with_sadness(
                        (
                            'Failed to remove patch from node.\n'
                            'Aborting patch removal attempt.\n'
                            'Error was: {error}'.format(
                                error=str(err),
                            )
                        ),
                        action,
                    )
            # Confirming health of cluster member
            try:
                if removed_from_this_node:
                    if get_manager_version() >= Version('5.0.5'):
                        # Wait for healthy services, and rollback if they do
                        # not become healthy
                        wait_for_healthy_services(**rollback_args)
                    else:
                        # If no other nodes needed modifying, the leader will
                        # still be the original leader, so we must switch away
                        # then back
                        if member == leader \
                           and removal_not_required == len(members) - 1:
                            say(
                                "Switching away from old leader temporarily "
                                "to confirm services are healthy after "
                                "modification."
                            )
                            change_cluster_leader(members[0])
                        change_cluster_leader(member)
            except ClusterLeaderChangeFailure as err:
                sadly_say(
                    "Failed to switch to {member}.\n"
                    "Attempting to roll back patch on this manager.".format(
                        member=member,
                    )
                )
                rollback(**rollback_args)
                exit_with_sadness(
                    (
                        "Rolled back {member}. Patch has not been fully "
                        "removed from cluster.\n"
                        "Please verify cluster healthy before re-attempting "
                        "this action."
                    ),
                    action,
                )
            except ClusterLeaderChangeToCurrent as err:
                exit_with_sadness(
                    (
                        "Failure attempting to change leader to {member}, "
                        "because this node was already the leader.\n"
                        "Cluster appears to be undergoing unexpected leader "
                        "changes while patching.".format(
                            member=member,
                        )
                    ),
                    action,
                )
            except ServiceHealthFailure as err:
                exit_with_sadness(
                    (
                        "Failure applying patch to {member}. "
                        "Cluster health did not recover after patch was "
                        "applied.\n"
                        "Please verify cluster healthy before re-attempting "
                        "this action.".format(
                            member=member,
                        )
                    ),
                    action,
                )
        if removal_not_required == len(members):
            exit_with_sadness(
                'Patch was not found on any nodes in the cluster!',
                action,
            )
        else:
            say("Patch removed from cluster.")
    else:
        say("Attempting to remove patch from manager.")
        ssh_details = ssh_details[0]
        try:
            abort_on_unhealthy_services()
            abort_on_ssh_failure(
                ssh_details['connection_string'],
                ssh_details['ssh_port'],
                ssh_details['key_path'],
            )
            remove_patch(
                target_patch_id,
                skip_services,
                ssh_details,
                action,
            )
        except (
            ServiceHealthFailure,
            SSHCheckFailure,
            PatchNotFound,
            PatchRemovalBlocked,
            PatchDefinitionParseFailure,
            InvalidBackup,
            HashCheckFailure,
            RegistryGenerationFailure,
        ) as err:
            exit_with_sadness(str(err), action)


def remove_patch(target_patch_id, skip_services, ssh_details, action):
    manager_connection_string = ssh_details['connection_string']
    keyfile = ssh_details['key_path']
    ssh_port = ssh_details['ssh_port']

    say("Retrieving patch registry information.")
    try:
        registry = download_patch_registry(manager_connection_string,
                                           ssh_port, keyfile)
    except PatchRegistryParseFailure as err:
        exit_with_sadness(str(err), action)

    say("Getting target patch information...")
    current_patches = registry.get('current_patches')
    target_patch = None
    for patch in current_patches:
        if patch['patch_id'] == target_patch_id:
            say('Target patch information located.')
            target_patch = patch
            break
    if target_patch is None:
        all_patch_ids = [patch['patch_id'] for patch in current_patches]
        raise PatchNotFound(
            'Could not locate patch with ID {patch_id}.\n'
            'Currently installed patches were: {patches}\n'
            'For more details, run {prog} with the list subcommand.'.format(
                patch_id=target_patch_id,
                patches=', '.join(all_patch_ids),
                prog=sys.argv[0],
            ),
        )

    say("Checking for blocking patches...")
    blockers = get_uninstall_blockers(current_patches).get(target_patch_id)
    if blockers is not None:
        raise PatchRemovalBlocked(
            'Could not uninstall patch with ID {patch_id}.\n'
            'Patch uninstall blocked by patches: {blockers}'.format(
                patch_id=target_patch_id,
                blockers=', '.join(blockers),
            ),
        )
    say("No blockers, patch is valid candidate for uninstall.")

    say("Retrieving old patch definition.")
    with tempdir() as tmp_path:
        local_patch_definition = os.path.join(tmp_path, 'patch_definition')
        download(
            file_name='patch_definition',
            source_path=patch['patch_directory'],
            dest_path=local_patch_definition,
            manager_connection_string=manager_connection_string,
            ssh_port=ssh_port,
            keyfile=keyfile,
        )
        try:
            with open(local_patch_definition) as def_handle:
                old_definition = json.load(def_handle)
        except ValueError:
            raise PatchDefinitionParseFailure(
                'Could not parse old patch definition retrieved from '
                '{patch_def_location}.'.format(
                    patch_def_location=os.path.join(
                        patch['patch_directory'], 'patch_definition',
                    ),
                ),
            )

    say("Building removal definition.")
    backup_path = os.path.join(
        patch['patch_directory'],
        'backups',
    )
    new_definition = {
        'patches': [
            {'destinations': old_patch['destinations']}
            for old_patch in old_definition['patches']
        ],
        'affected_services': old_definition['affected_services'],
        'removed_patch_backups_path': backup_path,
    }

    hash_type = 'md5'
    if 'md5sums_before' in old_definition:
        new_definition['md5sums_after'] = old_definition['md5sums_before']
    if 'md5sums_after' in old_definition:
        new_definition['md5sums_before'] = old_definition['md5sums_after']
    if 'sha256sums_after' in old_definition:
        # In a patch definition with SHA256 hashes
        new_definition['sha256sums_after'] = (
            old_definition['sha256sums_before']
        )
        new_definition['sha256sums_before'] = (
            old_definition['sha256sums_after']
        )
        hash_type = 'sha256'

    say("Checking for correct files to restore.")
    files_to_delete = []
    files_to_restore = []
    files_to_skip = []
    hash_check_key = '{hash_type}sums_after'.format(hash_type=hash_type)
    alt_check_key = '{hash_type}sums_before'.format(hash_type=hash_type)
    for target_file, valid_hashes in new_definition[hash_check_key].items():
        target_backup_path = os.path.join(
            backup_path,
            target_file.lstrip('/'),
        )

        backup_hash = get_file_hash(
            file_path=target_backup_path,
            manager_connection_string=manager_connection_string,
            ssh_port=ssh_port,
            keyfile=keyfile,
            hash_type=hash_type,
        )
        if backup_hash not in valid_hashes:
            pre_applied = new_definition[alt_check_key][target_file]
            if backup_hash in pre_applied:
                # This was probably a synced file, ignore it
                say(
                    'Skipping {target} as it was patched before the patch '
                    'applied (e.g. the file was synced).'.format(
                        target=target_file,
                    )
                )
                files_to_skip.append(target_file)
                # We don't want to check the file if we're skipping it
                new_definition[hash_check_key].pop(target_file)
                new_definition[alt_check_key].pop(target_file)
                continue

            raise InvalidBackup(
                'File to be restored does not match any acceptable MD5sums.\n'
                'File to be restored was: {target_file}\n'
                'Hash was "{backup_hash}", but valid hashes were: '
                '{valid}'.format(
                    target_file=target_backup_path,
                    backup_hash=backup_hash,
                    valid=', '.join(valid_hashes),
                ),
            )

        if backup_hash == 'DOESNOTEXIST':
            files_to_delete.append(target_file)
        else:
            files_to_restore.append((target_backup_path, target_file))

    say("Validating current files.")
    check_hashes_before(new_definition, manager_connection_string,
                        ssh_port, keyfile, hash_type=hash_type)

    possibly_stop_services(new_definition, skip_services,
                           manager_connection_string, ssh_port, keyfile)

    say("Deploying removal directory.")
    with tempdir() as tmp_dir:
        temporary_definition_file = os.path.join(tmp_dir, 'patch_definition')
        with open(temporary_definition_file, 'w') as def_handle:
            def_handle.write(format_json(new_definition))
        patch_dirs = generate_patch_data_dir(
            patch_name='remove-' + patch['patch_name'],
            patch_definition=new_definition,
            timestamp=generate_timestamp(),
            manager_connection_string=manager_connection_string,
            ssh_port=ssh_port,
            keyfile=keyfile,
            definition_file=temporary_definition_file,
            action=action,
        )

    if files_to_delete:
        say("Removing files added by target patch.")
        for delete_file in files_to_delete:
            ssh(
                manager_connection_string,
                ssh_port,
                keyfile,
                'sudo rm -f {delete_file} '.format(
                    delete_file=delete_file,
                )
            )
            delete_file_base, extension = os.path.splitext(delete_file)
            if extension == '.py':
                ssh(
                    manager_connection_string,
                    ssh_port,
                    keyfile,
                    'sudo rm -f {pyartefacts}'.format(
                        pyartefacts=delete_file_base + '.py[co]',
                    )
                )

    if files_to_restore:
        say("Restoring files modified by target patch.")
        for file_backup, target_file in files_to_restore:
            ssh(
                manager_connection_string,
                ssh_port,
                keyfile,
                'sudo cp {backup} {restored}'.format(
                    backup=file_backup,
                    restored=target_file,
                )
            )
            restore_file_base, extension = os.path.splitext(target_file)
            if extension == '.py':
                ssh(
                    manager_connection_string,
                    ssh_port,
                    keyfile,
                    'sudo rm -f {pyartefacts}'.format(
                        pyartefacts=restore_file_base + '.py[co]',
                    )
                )

    backup_root = patch_dirs['backup_root']
    delete_root = patch_dirs['delete_root']
    check_hashes_after(new_definition,
                       manager_connection_string, ssh_port, keyfile,
                       backup_root, delete_root, skip_services,
                       hash_type=hash_type)

    possibly_start_services(new_definition, skip_services,
                            manager_connection_string, ssh_port, keyfile)

    wait_for_healthy_services(new_definition, backup_root, delete_root,
                              skip_services,
                              manager_connection_string, ssh_port, keyfile)

    say("Patch removed, updating registry.")

    remove_patch_from_registry(registry, target_patch_id,
                               manager_connection_string, ssh_port, keyfile)

    say("Patch removed successfully.")

    # Return arguments required for rollback to allow rollback in cluster
    # patching
    return {
        'manager_connection_string': manager_connection_string,
        'ssh_port': ssh_port,
        'patch_definition': new_definition,
        'backup_root': backup_root,
        'delete_root': delete_root,
        'keyfile': keyfile,
        'skip_services': skip_services,
    }


def generate_patch_id(patch_name, definition_hash):
    """Generate the ID of a patch. This method should match that used in the
    central registry update tool."""
    return patch_name + '-' + definition_hash


def add_patch_to_registry(registry, patch_id, timestamp,
                          patch_name, patch_path, patch_definition,
                          manager_connection_string, ssh_port, keyfile):
    current_patches = registry.get('current_patches', [])

    if 'sha256sums_after' in patch_definition:
        affected_files = patch_definition['sha256sums_after'].keys()
        modified_files = patch_definition['sha256sums_after']
    else:
        affected_files = patch_definition['md5sums_after'].keys()
        modified_files = patch_definition['md5sums_after']

    block_uninstall = []
    for existing_patch in current_patches:
        if any(affected_file in existing_patch['modified_files']
               for affected_file in affected_files):
            block_uninstall.append(existing_patch['patch_id'])

    new_patch = {
        'patch_id': patch_id,
        'patch_name': patch_name,
        'timestamp': timestamp,
        'patch_directory': patch_path,
        'modified_files': modified_files,
        'blocks_uninstall_of': block_uninstall,
        'description': patch_definition.get('description'),
        'node_types': patch_definition.get('node_types'),
    }

    current_patches.append(new_patch)

    registry['current_patches'] = current_patches

    update_registry(registry, manager_connection_string, ssh_port, keyfile)


def update_registry(registry, manager_connection_string, ssh_port, keyfile):
    with tempdir() as tmp_path:
        local_reg_path = os.path.join(tmp_path, PATCH_REGISTRY_FILE_NAME)

        try:
            with open(local_reg_path, 'w') as local_reg_handle:
                local_reg_handle.write(format_json(registry))
        except Exception as err:
            raise RegistryGenerationFailure(
                'FAILED TO GENERATE NEW REGISTRY!\n'
                'PATCH STATE HAS BEEN UPDATED BUT REGISTRY IS INCORRECT!\n'
                'Error was: {err}'.format(err=err),
            )

        upload(
            file_name=PATCH_REGISTRY_FILE_NAME,
            source_path=tmp_path,
            dest_path=PATCH_REGISTRY_PATH,
            manager_connection_string=manager_connection_string,
            ssh_port=ssh_port,
            keyfile=keyfile,
        )


def remove_patch_from_registry(registry, patch_id, manager_connection_string,
                               ssh_port, keyfile):
    current_patches = []
    for patch in registry['current_patches']:
        if patch['patch_id'] == patch_id:
            continue
        else:
            current_patches.append(patch)

    registry['current_patches'] = current_patches

    update_registry(registry, manager_connection_string, ssh_port, keyfile)


def get_uninstall_blockers(patches):
    blockers = {}
    for patch in patches:
        for blocked in patch.get('blocks_uninstall_of', []):
            if blocked not in blockers:
                blockers[blocked] = []
            blockers[blocked].append(patch['patch_id'])
    return blockers


def retrieve_manager_patch_details(verbose, action):
    ssh_details = get_profile_ssh_details(action)
    leader = None

    if get_manager_version() >= Version('5.0.5'):
        ssh_details = get_cluster_nodes(
            ssh_details,
            include_node_types=NODE_TYPES,
        )
        if len(ssh_details) > 1:
            leader = ssh_details[0]['cluster_node_name']
    else:
        if len(ssh_details) > 1:
            if verbose:
                say("Determining cluster leader.")

            leader, _, _ = get_cluster_members()

    manager_patches = {}

    for mgr_ssh_details in ssh_details:
        manager_connection_string = mgr_ssh_details['connection_string']
        keyfile = mgr_ssh_details['key_path']
        ssh_port = mgr_ssh_details['ssh_port']
        cluster_node_name = mgr_ssh_details['cluster_node_name']

        if verbose:
            if cluster_node_name:
                say(
                    "Getting installed patch details from {node}...".format(
                        node=cluster_node_name,
                    )
                )
            else:
                say("Getting installed patch details from manager...")
        try:
            abort_on_ssh_failure(manager_connection_string, ssh_port, keyfile,
                                 cluster_node_name=cluster_node_name,
                                 quiet=not verbose)
        except SSHCheckFailure as err:
            exit_with_sadness(str(err), action)
        try:
            registry = download_patch_registry(manager_connection_string,
                                               ssh_port, keyfile,
                                               quiet=not verbose)
        except PatchRegistryParseFailure as err:
            exit_with_sadness(str(err), action)
        manager_patches[cluster_node_name] = registry.get('current_patches',
                                                          [])

    return manager_patches, leader


def get_node_type_counts(action, replica_count):
    node_type_counts = {}
    # We will need these to determine correct numbers for nodes
    # that patches are expected to have been applied to
    ssh_details = get_profile_ssh_details(action)
    node_type_counts['db'] = len(
        get_cluster_nodes(
            ssh_details,
            include_node_types=['db'],
        )
    )
    node_type_counts['broker'] = len(
        get_cluster_nodes(
            ssh_details,
            include_node_types=['broker'],
        )
    )
    node_type_counts['manager'] = (
        replica_count
        - node_type_counts['db']
        - node_type_counts['broker']
    )
    return node_type_counts


def list_patches(json_output, verbose):
    action = 'LISTING PATCHES'

    manager_patches, leader = retrieve_manager_patch_details(verbose,
                                                             action)

    if get_manager_version() >= Version('5.0.5'):
        on_other_nodes = 'on_nodes'
    else:
        on_other_nodes = 'on_replicas'

    cluster = (
        leader
        or (
            get_manager_version() >= Version('5.0.5')
            and None not in manager_patches
        )
    )

    if json_output:
        if cluster:
            output_json(manager_patches)
        else:
            output_json(manager_patches[None])
    elif not verbose:
        if cluster:
            display = []
            patch_id_mapping = {}
            if get_manager_version() >= Version('5.0.5'):
                replica_count = len(manager_patches)
            else:
                replica_count = len(manager_patches) - 1
            patch_num = 0

            if get_manager_version() >= Version('5.0.5'):
                leader_patches = manager_patches.popitem()[1]
            else:
                leader_patches = manager_patches.pop(leader)

            for patch in leader_patches:
                patch_num += 1
                display.append({
                    'patch': patch_num,
                    'on_leader': True,
                    on_other_nodes: 0,
                    'patch_id': patch['patch_id'],
                    'blocks': patch['blocks_uninstall_of'],
                })
                if get_manager_version() >= Version('5.0.5'):
                    display[-1][on_other_nodes] = 1
                    display[-1]['applies_to'] = patch.get(
                        'node_types',
                        ['manager'],
                    )
                patch_id_mapping[patch['patch_id']] = patch_num

            for replica in manager_patches.values():
                for patch in replica:
                    if patch['patch_id'] in patch_id_mapping:
                        pos = patch_id_mapping[patch['patch_id']] - 1
                        display[pos][on_other_nodes] += 1
                    else:
                        patch_num += 1
                        display.append({
                            'patch': patch_num,
                            'on_leader': False,
                            on_other_nodes: 1,
                            'patch_id': patch['patch_id'],
                            'blocks': patch['blocks_uninstall_of'],
                        })
                        patch_id_mapping[patch['patch_id']] = patch_num
                        if get_manager_version() >= Version('5.0.5'):
                            display[-1]['applies_to'] = patch.get(
                                'node_types', ['manager']
                            )

            node_type_counts = None
            if get_manager_version() >= Version('5.0.5'):
                node_type_counts = get_node_type_counts(action, replica_count)

            for patch in display:
                if patch.get('applies_to'):
                    expected_nodes = sum(
                        node_type_counts[entry]
                        for entry in patch['applies_to']
                    )
                else:
                    expected_nodes = replica_count
                patch[on_other_nodes] = '{on}/{total}'.format(
                    on=patch[on_other_nodes],
                    total=expected_nodes,
                )
                patch['patch'] = str(patch['patch'])
                if get_manager_version() >= Version('5.0.5'):
                    patch['applies_to'] = ','.join(patch['applies_to'])

            fields = ['patch', 'on_leader', on_other_nodes, 'patch_id',
                      'blocks']
            if get_manager_version() >= Version('5.0.5'):
                fields.remove('on_leader')
                fields.append('applies_to')
        else:
            display = []
            patch_id_mapping = {}
            for num, patch in enumerate(manager_patches[None]):
                display.append({
                    'patch': str(num + 1),
                    'installed': True,
                    'patch_id': patch['patch_id'],
                    'blocks': patch['blocks_uninstall_of'],
                })
                patch_id_mapping[patch['patch_id']] = str(num + 1)

            fields = ['patch', 'installed', 'patch_id', 'blocks']

        # Provide short patch blocking output
        for blocker in display:
            if blocker['blocks']:
                blocks = []
                for patch in display:
                    if patch['patch_id'] in blocker['blocks']:
                        blocks.append(patch['patch'])
                blocker['blocks'] = ', '.join(blocks)

        output_table(display, fields)
    else:
        for patches in manager_patches.values():
            uninstall_blocked = get_uninstall_blockers(patches)
            for patch in patches:
                if patch['patch_id'] in uninstall_blocked:
                    patch['blockers'] = uninstall_blocked[patch['patch_id']]
                else:
                    patch['blockers'] = []

        # Determine if any of the cluster nodes have different patches
        if leader:
            node_groups = []
            for node, patches in manager_patches.items():
                matches_others = False
                # If we compare with timestamps included it is easy to get
                # false positives
                node_timestamps = [
                    patch.pop('timestamp') for patch in patches
                ]
                # Strip out patch directories as they contain the timestamp
                # and we don't display them in 'friendly' output anyway
                for patch in patches:
                    patch.pop('patch_directory')
                for grp_nodes, grp_patches, grp_timestamps in node_groups:
                    if patches == grp_patches:
                        grp_nodes.append(node)
                        grp_timestamps.append(node_timestamps)
                        matches_others = True
                if not matches_others:
                    node_groups.append(([node], patches, [node_timestamps]))

            # Sort the groups, putting the most frequently found patch
            # combinations first, then by amount of patches if there is a
            # collision
            node_groups.sort(key=lambda x: (len(x[0]), len(x[1])),
                             reverse=True)
            # And then put the group with the leader at the top of the list
            for idx, entry in enumerate(node_groups):
                if leader in entry[0]:
                    leader_group_pos = idx
                    break
            node_groups.insert(
                0,
                node_groups.pop(leader_group_pos),
            )

            # Merge the timestamps back in
            for grp_nodes, grp_patches, grp_timestamps in node_groups:
                for patch_num in range(len(grp_patches)):
                    grp_patches[patch_num]['timestamp'] = [
                        grp_timestamps[node_num][patch_num] +
                        '(' + grp_nodes[node_num] + ')'
                        for node_num in range(len(grp_nodes))
                    ]

            display_patches = node_groups[0][1]
            differences = node_groups[1:]
        else:
            display_patches = manager_patches[None]
            differences = []

        if display_patches:
            say('')
            if leader:
                say("Cluster patches:")
            display_patch_listing(display_patches)
        else:
            if leader:
                say('')
                say("Cluster patches:")
                if differences:
                    say("No installed patches found on leader.")
                else:
                    say("No installed patches found on cluster.")
            else:
                say("No installed patches found on manager.")

        for different_nodes, different_patches, _ in differences:
            sadly_say('')
            if len(different_nodes) > 1:
                sadly_say('Replicas {nodes} have different patches:'.format(
                    nodes=', '.join(different_nodes),
                ))
            else:
                sadly_say('Replica {node} has different patches:'.format(
                    node=different_nodes[0],
                ))
            if different_patches:
                display_patch_listing(different_patches,
                                      output_method=sadly_say)
            else:
                sadly_say('No patches installed.')
            sadly_say('')

        if differences:
            sadly_say(
                "Please try to resolve patch issues by applying or removing "
                "patches from the cluster."
            )
            sys.exit(1)


def display_patch_listing(patches, output_method=say):
    for patch in patches:
        if patch['blockers']:
            uninstall_restrictions = (
                ' Uninstall blocked by: {blockers}'.format(
                    blockers=', '.join(patch['blockers']),
                )
            )
        else:
            uninstall_restrictions = ''

        if patch.get('description'):
            description = (
                ' Patch description: {description}'.format(
                    description=patch['description'],
                )
            )
        else:
            description = ''

        if isinstance(patch['timestamp'], list):
            display_timestamp = ', '.join(patch['timestamp'])
        else:
            display_timestamp = patch['timestamp']

        output_method(
            'Patch {name} (ID: {patch_id}) was installed on '
            '{timestamp}.{description}{uninstall_restrictions}'.format(
                name=patch['patch_name'],
                patch_id=patch['patch_id'],
                timestamp=display_timestamp,
                uninstall_restrictions=uninstall_restrictions,
                description=description,
            )
        )


def download_patch_registry(manager_connection_string, ssh_port, keyfile,
                            quiet=False):
    if not quiet:
        say('Checking patch registry.')
    patch_registry_md5 = get_file_hash(
        PATCH_REGISTRY_PATH,
        manager_connection_string,
        ssh_port,
        keyfile,
        'md5',
    )
    if patch_registry_md5 == 'DOESNOTEXIST':
        if not quiet:
            say('No registry found.')
        registry = {
            'current_patches': [],
        }
    else:
        with tempdir() as tmp_path:
            local_reg_path = os.path.join(tmp_path, PATCH_REGISTRY_FILE_NAME)

            download(
                file_name=PATCH_REGISTRY_FILE_NAME,
                source_path=BASE_MANAGER_PATCH_STORAGE_PATH,
                dest_path=local_reg_path,
                manager_connection_string=manager_connection_string,
                ssh_port=ssh_port,
                keyfile=keyfile,
                quiet=quiet,
            )

            try:
                with open(local_reg_path) as local_reg_handle:
                    registry = json.load(local_reg_handle)
            except ValueError:
                raise PatchRegistryParseFailure(
                    'Could not parse existing patch registry.',
                )
    return registry


def determine_central_registry_location(central_repository_location,
                                        manager_version):
    return (
        central_repository_location + str(manager_version) + '.json'
    )


def retrieve_central_patch_listing(registry_location, action,
                                   manager_version,
                                   verbose):
    if verbose:
        say('Retrieving central patch registry.')
    try:
        raw_listing = urllib2.urlopen(
            registry_location,
        ).read()
    except urllib2.HTTPError as err:
        exit_with_sadness(
            (
                'Failed to retrieve central registry from: {location}\n'
                'Failed with {code}: {reason}.'.format(
                    location=registry_location,
                    code=err.code,
                    reason=err.reason,
                )
            ),
            action,
        )

    try:
        central_listing = json.loads(raw_listing)
    except ValueError as err:
        exit_with_sadness(
            (
                'Failed to parse central registry as json.\n'
                'Error was: {err}\n'
                'Raw central registry was: {raw}'.format(
                    err=str(err),
                    raw=raw_listing,
                )
            ),
            action,
        )

    if manager_version.edition == 'community':
        if verbose:
            say('Filtering out premium-only patches.')
        central_listing['patches'] = [
            patch for patch in central_listing['patches']
            if patch['supports_community']
        ]

    return central_listing


def apply_central_manager_patches(central_repository_location,
                                  repo_root_location, limited_install_count,
                                  install_patch_command):
    action = "APPLYING RECOMMENDED PATCHES"

    say('Getting manager version to download correct patches.')
    manager_version = get_manager_version()

    central_registry_location = determine_central_registry_location(
        central_repository_location, manager_version,
    )

    central_listing = retrieve_central_patch_listing(
        central_registry_location,
        action,
        manager_version,
        verbose=True,
    )

    say('Determining which patches need installing.')
    _, leader = set_recommended_patch_status(
        central_listing,
        action,
        verbose=True,
    )
    # Filter out the patches that are not missing from any managers in this
    # profile
    central_listing['patches'] = [
        patch for patch in central_listing['patches']
        if patch['missing_on_manager']
    ]

    if leader:
        target = 'cluster'
    else:
        target = 'manager'

    if len(central_listing['patches']) == 0:
        say(
            'No patches to install, the {target} has all recommended '
            'patches.'.format(target=target)
        )
        sys.exit(0)

    if limited_install_count is None:
        limited_install_count = len(central_listing['patches'])
    elif limited_install_count == 0:
        exit_with_sadness(
            (
                'Cannot install 0 patches. Please set a higher install limit.'
            ),
            action,
        )

    with tempdir() as tmp_path:
        if repo_root_location and repo_root_location.startswith('file://'):
            say('Patches are already locally stored. Skipping download.')
            local_patch_storage = repo_root_location[len('file://'):]
        else:
            local_patch_storage = tmp_path
            download_central_manager_patches(
                central_repository_location,
                repo_root_location,
                destination=tmp_path,
                action=action,
                central_listing=central_listing,
            )

        installed_patches = 0
        for patch in central_listing['patches'][:limited_install_count]:
            patch_definition = os.path.join(
                local_patch_storage, patch['patch_definition_path']
            )
            say('Applying {patch}'.format(patch=patch['patch_name']))
            patch_applier(
                patch_definition,
                skip_services=False,
                install_patch_command=install_patch_command,
                skip_version_check=True,  # We already checked this
            )
            say('Applied {patch}'.format(patch=patch['patch_name']))
            installed_patches += 1
        say(
            'Successfully installed {num} patches.'.format(
                num=installed_patches,
            )
        )
        if installed_patches == len(central_listing['patches']):
            say(
                'All patches are installed, the {target} has all '
                'recommended patches!'.format(target=target)
            )
        else:
            say(
                'There are still {count} recommended patches that should be '
                'installed.'.format(
                    count=str(
                        len(central_listing['patches']) - installed_patches
                    ),
                )
            )


def download_central_manager_patches(
    central_repository_location,
    repo_root_location,
    destination,
    action='DOWNLOADING RECOMMENDED PATCHES',
    central_listing=None,
):
    say('Checking and preparing destination directory.')
    # We won't just create it with mkdir -p because that won't work on windows
    if not os.path.isdir(destination):
        exit_with_sadness(
            (
                'Destination directory does not exist or is not a directory!'
            ),
            action,
        )
    patch_destination = os.path.join(destination, 'patches')
    if not os.path.exists(patch_destination):
        os.mkdir(patch_destination)

    if not central_listing:
        say('Getting manager version to download correct patches.')
        manager_version = get_manager_version()

        central_registry_location = determine_central_registry_location(
            central_repository_location, manager_version,
        )

        central_listing = retrieve_central_patch_listing(
            central_registry_location,
            action,
            manager_version,
            verbose=True,
        )

    say('Determining all required file locations.')
    repo_root = repo_root_location or central_listing['patch_root_path']
    definition_files = []
    patch_files = []
    for patch in central_listing['patches']:
        definition_files.append(patch['patch_definition_path'])
        for patch_file in patch['patch_files']:
            patch_files.append(patch_file)

    say('Downloading definitions')
    for definition_file in definition_files:
        file_data = urllib2.urlopen(repo_root + definition_file).read()
        with open(os.path.join(destination, definition_file), 'w') as fh:
            fh.write(file_data)

    say('Downloading patch files')
    repo_root += 'patches/'
    for patch_file in patch_files:
        file_data = urllib2.urlopen(repo_root + patch_file).read()
        with open(os.path.join(patch_destination, patch_file), 'w') as fh:
            fh.write(file_data)

    say(
        'All patches downloaded to {destination}'.format(
            destination=destination
        )
    )


def set_recommended_patch_status(central_listing, action,
                                 verbose):
    if verbose:
        say('Getting current patch state from manager.')
    manager_patches, leader = retrieve_manager_patch_details(
        verbose,
        action,
    )
    central_patches = set(
        patch['patch_id'] for patch in central_listing['patches']
    )

    patch_id_mapping = {}
    for patch_idx, patch in enumerate(central_listing['patches']):
        patch['missing_on_manager'] = []
        if get_manager_version() >= Version('5.0.5') and leader:
            patch['missing_on_current_leader'] = False
        patch_id_mapping[patch['patch_id']] = patch_idx

    for manager in manager_patches:
        installed_on_manager = set(
            patch['patch_id'] for patch in manager_patches[manager]
        )
        missing_patches = central_patches - installed_on_manager

        for patch in missing_patches:
            patch = central_listing['patches'][patch_id_mapping[patch]]
            patch['missing_on_manager'].append(manager)
            if get_manager_version() >= Version('5.0.5'):
                if leader and manager == leader:
                    patch['missing_on_current_leader'] = True

    # Return these for comparisons with missing patch details
    return manager_patches, leader


def list_central_manager_patches(central_repository_location,
                                 skip_checking_manager,
                                 json_output, verbose):
    action = 'LISTING CENTRAL PATCHES'

    if get_manager_version() >= Version('5.0.5'):
        on_other_nodes = 'on_nodes'
    else:
        on_other_nodes = 'on_replicas'

    if verbose:
        say('Getting manager version to list correct patches.')
    manager_version = get_manager_version()

    central_registry_location = determine_central_registry_location(
        central_repository_location, manager_version,
    )

    central_listing = retrieve_central_patch_listing(
        central_registry_location,
        action,
        manager_version,
        verbose,
    )

    if skip_checking_manager:
        say('Not checking patches on manager.')
        leader = None
    else:
        manager_patches, leader = set_recommended_patch_status(
            central_listing,
            action,
            verbose,
        )

    if json_output:
        output_json(central_listing)
    elif not verbose:
        display = []

        replica_count = len(manager_patches)

        node_type_counts = None
        if get_manager_version() >= Version('5.0.5'):
            node_type_counts = get_node_type_counts(action, replica_count)

        for patch in central_listing['patches']:
            display_patch = {
                'patch_id': patch['patch_id'],
            }

            missing_patches = patch['missing_on_manager']
            if leader:
                installed_on_leader = leader not in missing_patches

                missing_on_replicas = len(missing_patches)
                if get_manager_version() < Version('5.0.5'):
                    if not installed_on_leader:
                        missing_on_replicas -= 1
                    replica_count -= 1
                installed_on_replicas = replica_count - missing_on_replicas

                if get_manager_version() >= Version('5.0.5'):
                    for node_patches in manager_patches.values():
                        for node_patch in node_patches:
                            if node_patch['patch_id'] == patch['patch_id']:
                                display_patch['applies_to'] = node_patch.get(
                                    'node_types', ['manager']
                                )

                if display_patch.get('applies_to'):
                    expected_nodes = sum(
                        node_type_counts[entry]
                        for entry in display_patch['applies_to']
                    )
                else:
                    expected_nodes = replica_count

                display_patch['on_leader'] = installed_on_leader
                display_patch[on_other_nodes] = '{on}/{total}'.format(
                    on=installed_on_replicas,
                    total=expected_nodes,
                )

                fields = ['on_leader', on_other_nodes, 'patch_id']
                if get_manager_version() >= Version('5.0.5'):
                    fields.remove('on_leader')
                    fields.append('applies_to')
            else:
                display_patch['installed'] = not missing_patches
                fields = ['installed', 'patch_id']

            display.append(display_patch)
        output_table(display, fields)
    else:
        say('')
        say(
            'Recommended patches for {version}:'.format(
                version=manager_version,
            )
        )
        missing_patches = False
        for patch in central_listing['patches']:
            patch_message = '  '
            if leader:
                if len(patch['missing_on_manager']) == len(manager_patches):
                    missing_patches = True
                    patch_message += '(PATCH NOT INSTALLED ON CLUSTER)'
                elif patch['missing_on_manager']:
                    missing_patches = True
                    if patch['missing_on_current_leader']:
                        patch_message += (
                            '(PATCH MISSING FROM LEADER, BUT PRESENT ON '
                            'ONE OR MORE REPLICAS)'
                        )
                    else:
                        patch_message += (
                            '(PATCH INSTALLED ON LEADER, BUT MISSING FROM '
                            'ONE OR MORE REPLICAS)'
                        )
            else:
                # If we aren't a cluster then having the patch missing on any
                # manager means it's missing on the only manager
                if patch['missing_on_manager']:
                    missing_patches = True
                    patch_message += '(PATCH NOT INSTALLED ON MANAGER)'
            patch_message += (
                'Patch {name} (ID: {patch_id}). Description: {description}'
            )
            say(
                patch_message.format(
                    name=patch['patch_name'],
                    patch_id=patch['patch_id'],
                    description=patch['patch_description'],
                )
            )

        if (not missing_patches) and (not skip_checking_manager):
            say('')
            say('All recommended patches are installed.')


def central_repo_location(value):
    """Make sure central repo locations end in a slash, because we know we
    will fail later if they don't. Other than that, urllib2 will request the
    location later, so we'll let it handle other failures."""
    if not value.endswith('/'):
        value += '/'
    return value


def add_apply_and_remove_args(parser):
    parser.add_argument(
        '-s', '--skip-services',
        help=(
            'Set this flag to not restart services. This is only to be used '
            'when instructed by Cloudify support.'
        ),
        action='store_true',
        default=False,
        required=False,
    )
    parser.add_argument(
        '-S', '--skip-version-check',
        help=(
            'Set this flag to not check manager version. This is only to be '
            'used when instructed by Cloudify support.'
        ),
        action='store_true',
        default=False,
        required=False,
    )


def add_manager_updates_args(parser, repo_root=True):
    parser.add_argument(
        '-c', '--central-repository-location',
        help=(
            'Location of central patch listings. If this is a local file, it '
            'must be specified as file:///path/to/central_registry_root/.'
        ),
        default=(
            'https://raw.githubusercontent.com/'
            'cloudify-cosmo/patchify/master/central_registry/'
        ),
        type=central_repo_location,
    )

    if repo_root:
        parser.add_argument(
            '-r', '--repo-root-location',
            help=(
                'Location to retrieve patch files from. '
                'If not set, the location will be retrieved with the patch '
                'listings.'
            ),
            type=central_repo_location,
        )


def add_list_output_args(parser):
    exclusive_options = parser.add_mutually_exclusive_group()
    exclusive_options.add_argument(
        '-j', '--json',
        help=(
            'Whether to output the patch list in json.'
            'This option may not be used with the verbose output option.'
        ),
        action='store_true',
        default=False,
        required=False,
    )
    exclusive_options.add_argument(
        '-v', '--verbose',
        help=(
            'Output extra details for these patches. '
            'This option may not be used with the json output option.'
        ),
        action='store_true',
        default=False,
        required=False,
    )


def add_install_patch_command_arg(parser):
    parser.add_argument(
        '-P', '--install-patch-command',
        help=(
            'Set this flag to yum install the patch command if it is not '
            'present.'
        ),
        action='store_true',
        default=False,
        required=False,
    )


def add_logger_arg(parser):
    parser.add_argument(
        '-L', '--log-path',
        help=(
            'The path to log patchify information to. '
            'To avoid logging to file, set this as an empty string (e.g. "").'
        ),
        default=get_log_path(),
        required=False,
    )


def nullable_int(value):
    if value is not None:
        try:
            value = int(value)
        except ValueError:
            raise argparse.ArgumentTypeError(
                # The null is a default
                'Expected an integer input.'
            )
    return value


def main():
    abort_unless_profile_dir_exists()

    parser = argparse.ArgumentParser(
        description=(
            'Apply a patch to a cloudify manager.\n' +
            get_patchify_version_output()
        ),
    )

    parser.add_argument(
        '-v', '--version',
        help="Display version information.",
        action='version',
        version=get_patchify_version_output(),
    )

    subparsers = parser.add_subparsers(help='Patch action',
                                       dest='action')

    list_patch_args = subparsers.add_parser('list',
                                            help='List installed patches')
    add_list_output_args(list_patch_args)
    add_logger_arg(list_patch_args)

    apply_patch_args = subparsers.add_parser('apply',
                                             help='Apply a patch')
    apply_patch_args.add_argument(
        '-p', '--patch_file',
        help=(
            'The patch file to use. Note that for POC purposes this is just '
            'a definition file, but this should probably be changed to a zip '
            'for real world use (so that it can work on Windows as well as '
            'Linux.'
        ),
        required=True,
    )
    add_install_patch_command_arg(apply_patch_args)
    add_apply_and_remove_args(apply_patch_args)
    add_logger_arg(apply_patch_args)

    remove_patch_args = subparsers.add_parser('remove',
                                              help='Remove a patch')
    remove_patch_args.add_argument(
        '-p', '--patch-id',
        help=(
            'The ID of the patch to remove. This can be obtained using the '
            'list subcommand.'
        ),
        required=True,
    )
    add_apply_and_remove_args(remove_patch_args)
    add_logger_arg(remove_patch_args)

    manager_updates = subparsers.add_parser(
        'manager-updates',
        help='Check, apply, or download recommended updates.'
    )
    manager_updates_subparsers = manager_updates.add_subparsers(
        dest='subaction'
    )
    list_mgr_updates_args = manager_updates_subparsers.add_parser(
        'list',
        help=(
            'List recommended manager patches and show whether they are '
            'installed on the currently active cfy profiles.'
        ),
    )
    list_mgr_updates_args.add_argument(
        '-s', '--skip-checking-manager',
        help=(
            'Skip checking manager patches. Only list possible patches for '
            'this manager version.'
        ),
        default=False,
        action='store_true',
    )
    add_list_output_args(list_mgr_updates_args)
    add_manager_updates_args(list_mgr_updates_args, repo_root=False)
    add_logger_arg(list_mgr_updates_args)

    download_mgr_updates_args = manager_updates_subparsers.add_parser(
        'download',
        help=(
            'Download all recommended manager patches for the manager '
            'version in the currently active cfy profile.'
        ),
    )
    download_mgr_updates_args.add_argument(
        '-d', '--destination',
        help=(
            'Destination directory for downloaded patches. This must be a '
            'writeable location for the current user.'
        ),
        required=True,
    )
    add_manager_updates_args(download_mgr_updates_args)
    add_logger_arg(download_mgr_updates_args)

    apply_mgr_updates_args = manager_updates_subparsers.add_parser(
        'apply',
        help=(
            'Apply some or all recommended updates for the manager in the '
            'currently active cfy profile. These patches will be installed '
            'in the order they are listed in the manager-updates list.'
        )
    )
    apply_mgr_updates_args.add_argument(
        '-l', '--limit-install-count',
        help=(
            'Only install some of the specified patches. This number of '
            'patches that are currently not installed will be installed.'
        ),
        type=nullable_int,
        default=None,
    )
    add_install_patch_command_arg(apply_mgr_updates_args)
    add_manager_updates_args(apply_mgr_updates_args)
    add_logger_arg(apply_mgr_updates_args)

    args = parser.parse_args()

    if hasattr(args, 'log_path') and args.log_path:
        global PATCHIFY_LOG_PATH
        PATCHIFY_LOG_PATH = args.log_path
        log_size = 0
        units = 'B'
        if os.path.exists(PATCHIFY_LOG_PATH):
            log_size = os.path.getsize(PATCHIFY_LOG_PATH)
            if log_size > (1024 * 1024):
                log_size = log_size / (1024.0 * 1024)
                units = 'MiB'
            elif log_size > 1024:
                log_size = log_size / 1024.0
                units = 'KiB'
        say(
            'Log file {path} is currently {size:.2f}{units}'.format(
                path=PATCHIFY_LOG_PATH,
                size=log_size,
                units=units
            )
        )

    if args.action == 'apply':
        patch_applier(args.patch_file, args.skip_services,
                      args.install_patch_command, args.skip_version_check)
    elif args.action == 'remove':
        patch_remover(args.patch_id, args.skip_services)
    elif args.action == 'list':
        list_patches(args.json, args.verbose)
    elif args.action == 'manager-updates' and args.subaction == 'list':
        list_central_manager_patches(
            args.central_repository_location,
            args.skip_checking_manager,
            args.json,
            args.verbose,
        )
    elif args.action == 'manager-updates' and args.subaction == 'download':
        download_central_manager_patches(
            args.central_repository_location,
            args.repo_root_location,
            args.destination,
        )
    elif args.action == 'manager-updates' and args.subaction == 'apply':
        apply_central_manager_patches(
            args.central_repository_location,
            args.repo_root_location,
            args.limit_install_count,
            args.install_patch_command,
        )
    else:
        # You can't get here without messing about with the code above
        raise RuntimeError('Invalid action specified in parser.')


if __name__ == '__main__':
    main()
