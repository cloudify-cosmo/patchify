#! /usr/bin/env python
# Note that cfy is used rather than the client so that the user does not have
# to provide credentials, etc, to this script.
from __future__ import print_function
import argparse
from contextlib import contextmanager
import datetime
import json
import os
import subprocess
import sys
import tempfile
import time

BASE_MANAGER_PATCH_STORAGE_PATH = '/etc/cloudify/patches'
PATCH_REGISTRY_FILE_NAME = 'patch_registry.json'
PATCH_REGISTRY_PATH = os.path.join(
    BASE_MANAGER_PATCH_STORAGE_PATH, PATCH_REGISTRY_FILE_NAME
)


class VersionError(Exception):
    pass


class UploadError(Exception):
    pass


class DownloadError(Exception):
    pass


class ClusterUnhealthy(Exception):
    pass


class ClusterLeaderChangeFailure(Exception):
    pass


class ClusterLeaderChangeToCurrent(Exception):
    pass


class SSHCheckFailure(Exception):
    pass


class ServiceHealthFailure(Exception):
    pass


class PatchNotFound(Exception):
    pass


class PatchRemovalBlocked(Exception):
    pass


class PatchDefinitionParseFailure(Exception):
    pass


class InvalidBackup(Exception):
    pass


class PatchRegistryParseFailure(Exception):
    pass


class MD5CheckFailure(Exception):
    pass


class RegistryGenerationFailure(Exception):
    pass


class PatchApplyFailed(Exception):
    pass


class PatchAlreadyApplied(Exception):
    pass


def exit_with_sadness(message, action):
    sadly_say(message)
    sadly_say('FAILED {action}!'.format(action=action))
    sys.exit(1)


def say(message):
    print(message)


def sadly_say(message):
    sys.stderr.write('%s\n' % message)


def get_manager_version():
    try:
        version_output = subprocess.check_output(['cfy', '--version'])
    except subprocess.CalledProcessError:
        return None

    community = 'Community edition' in version_output
    version = None

    for line in version_output.splitlines():
        if line.lower().startswith('cloudify manager'):
            # Expecting line to be similar to:
            # Cloudify Manager 4.2.0 [ip=10.239.3.199]
            version = line.split(' ')[2]
            break

    return {
        'version': version,
        'community': community,
        'raw': version_output,
    }


def parse_version_string(version):
    """
        Parse a version string in semantic versioning format.
    """
    version = version.split('.')

    if len(version) != 3:
        raise VersionError(
            'Version string must contain three components. '
            'Found: {split_version}'.format(
                split_version=version,
            ),
        )

    try:
        version = tuple(int(part) for part in version)
    except ValueError:
        raise VersionError(
            'Version string must contain only integer components. '
            'Found: {version}'.format(version=version),
        )

    return version


def load_patch_definition(filename):
    with open(filename) as definition_handle:
        definition = json.load(definition_handle)

    # Currently this only handles version 1.0.0 of patch definitions:
    # {
    #   "patch_version": 1.0.0,  # No other value supported yet
    #   "manager_versions": [4.2],  # List of supported cloudify managers
    #   "community": <true|false>,  # Whether community is supported
    #   "premium": <true|false>,  # Whether premium is supported
    #   "patches": [  # List of patch files to apply.
    #                 # These should be in a format 'patch' on centos
    #                 # understands
    #     {
    #       "patch_file": "<patch file name in patch archive 'patches' dir>",
    #       "md5sum": "<md5sum of patch file>",
    #       "destinations": [  # List of locations that this file should patch
    #         "/path/to/location/on/manager",
    #         "/path/to/other/location/on/manager",
    #       ]
    #     },
    #     ... # More patch files, if any are required
    #   ],
    #   "affected_services": [  # List of services that will be stopped before
    #                           # applying the patches and started afterwards.
    #                           # These services will also be stopped before
    #                           # any rollbacks and started afterwards.
    #     "cloudify-service-1",
    #     "cloudify-service-2"
    #   ],
    #   # Following sections are optional, but recommended
    #   "md5sums_before": {  # Each file will have md5sum checked before
    #                        # applying the patches. If a file on the
    #                        # manager is specified in this list and does not
    #                        # have an md5sum in the acceptable list for that
    #                        # file, the patch process will be aborted.
    #                        # Files in this list will be backed up before the
    #                        # patch is applied, and restored if it aborts for
    #                        # any reason after the initial checks.
    #     "/path/to/file1": ["<md5sum before patch is applied>"],
    #     "/path/to/file2": ["<md5sum before patch is applied>"],
    #     "/path/to/file3": ["<md5sum before patch is applied>",
    #                        "<other acceptable md5sum>"]
    #   },
    #   "md5sums_after": {  # After all commands have been run, these md5sums
    #                       # will be checked after all patches have been
    #                       # applied and will abort and roll back the patch.
    #                       # While the list doesn't have to be the same, it
    #                       # probably should be, usually.
    #     "/path/to/file1": ["<md5sum after patch is applied>"],
    #     "/path/to/file2": ["<md5sum after patch is applied>"],
    #     "/path/to/file3": ["<md5sum after patch is applied>",
    #                        "<other acceptable md5sum>"]
    #   }
    # }
    try:
        patch_version = parse_version_string(definition['patch_version'])
    except KeyError:
        add_definition_error(
            message=(
                'patch_version not found in definition. '
                'Please confirm patch is valid and not corrupted.'
            ),
            definition=definition,
            fatal=True,
        )
        return definition

    if patch_version > (1, 1, 0):
        add_definition_error(
            message=(
                'Only version 1.1.0 and earlier patches are supported '
                'currently. Found version: {version}'.format(
                    version='.'.join([str(i) for i in patch_version]),
                )
            ),
            definition=definition,
            fatal=True,
        )
        return definition

    required_keys = set(['manager_versions', 'community', 'premium',
                         'patch_version'])
    recommended_keys = set()
    if patch_version[0] == 1:
        required_keys.update(['patches', 'affected_services'])
        recommended_keys.update(['md5sums_before', 'md5sums_after'])
        if patch_version[1] >= 1:
            required_keys.add('description')

    missing_keys = required_keys - set(definition)
    if missing_keys:
        add_definition_error(
            message=(
                'Patch definition must contain {keys}. '
                'Keys found: {found}'.format(
                    keys=', '.join(missing_keys),
                    found=', '.join(definition.keys()),
                )
            ),
            definition=definition,
            fatal=True,
        )

    missing_recommended = recommended_keys - set(definition)
    if missing_recommended:
        add_definition_error(
            message=(
                'Patch definition should contain {keys}. '
                'Keys found: {found}'.format(
                    keys=', '.join(missing_recommended),
                    found=', '.join(definition.keys()),
                )
            ),
            definition=definition,
        )

    # Unexpected keys might cause interesting behaviour. We'll avoid that.
    allowed_keys = required_keys | recommended_keys
    allowed_keys.add('errors')
    unsupported_keys = set(definition) - allowed_keys
    if unsupported_keys:
        add_definition_error(
            message=(
                "Patch definition version {version} must not contain "
                "{keys}. Supported keys are: {supported}".format(
                    keys=', '.join(unsupported_keys),
                    version=definition['patch_version'],
                    supported=', '.join(allowed_keys),
                )
            ),
            definition=definition,
            fatal=True,
        )

    if definition.get('description'):
        # To make anything that displays the description with any other
        # information (such as display_patch) easier to structure.
        if not definition['description'].endswith('.'):
            add_definition_error(
                message='Patch description must end with a full stop.',
                definition=definition,
                fatal=True,
            )

    return definition


def load_and_validate_patch_definition(filename, action):
    definition = load_patch_definition(filename)

    fatal_error = False
    for error in definition.get('errors', []):
        message = error['message']
        if error['fatal']:
            message = 'FATAL: ' + message
            fatal_error = True
        sadly_say(message)
    if fatal_error:
        exit_with_sadness('Patch definition error!', action)

    return definition


def add_definition_error(message, definition, fatal=False):
    errors = definition.get('errors', [])
    errors.append({
        'message': message,
        'fatal': fatal,
    })
    definition['errors'] = errors


def get_unhealthy_services():
    try:
        _, results = get_cfy_output(['status'])
    except subprocess.CalledProcessError:
        return ['All processes (is cfy pointing at a working manager?)']

    unhealthy_services = []

    for result in results:
        service = result[1]
        status = result[2]
        if status not in ('running', 'up'):
            unhealthy_services.append(service)

    return unhealthy_services


def get_profile_ssh_details(action):
    try:
        headers, results = get_cfy_output(['profiles', 'show-current'])
    except subprocess.CalledProcessError as err:
        exit_with_sadness(
            (
                'Failed to retrieve SSH details from profile, with error: '
                '{err}'
            ).format(
                err=str(err),
            ),
            action,
        )

    ssh_user_pos = headers.index('ssh_user')
    manager_ip_pos = headers.index('manager_ip')
    ssh_port_pos = headers.index('ssh_port')
    ssh_key_path_pos = headers.index('ssh_key_path')
    try:
        cluster_node_name_pos = headers.index('cluster node name')
    except ValueError:
        cluster_node_name_pos = None

    manager_connection_details = []
    for result in results:
        manager_connection_details.append({
            'manager_ip': result[manager_ip_pos],
            'connection_string': (
                result[ssh_user_pos] + '@' + result[manager_ip_pos]
            ),
            'ssh_port': result[ssh_port_pos],
            'key_path': result[ssh_key_path_pos],
            'cluster_node_name': (
                result[cluster_node_name_pos]
                if cluster_node_name_pos else None
            ),
        })

    return manager_connection_details


def get_cluster_members():
    headers, results = get_cfy_output(['cluster', 'nodes', 'list'])

    state_pos = headers.index('state')
    name_pos = headers.index('name')

    leader_node = None
    member_nodes = []
    offline_nodes = []
    for result in results:
        node_name = result[name_pos]
        node_state = result[state_pos]
        if node_state == 'leader':
            if leader_node is not None:
                raise ClusterUnhealthy(
                    'Found multiple leader nodes in the cluster. This should '
                    'not happen! Both {leader1} and {leader2} had their '
                    'state set to "leader".'.format(
                        leader1=leader_node,
                        leader2=node_name,
                    ),
                )
            leader_node = node_name
        elif node_state == 'replica':
            member_nodes.append(node_name)
        elif node_state == 'offline':
            offline_nodes.append(node_name)
        else:
            raise ClusterUnhealthy(
                'Unexpected cluster node state found: '
                '{node} is in state {state}.'.format(
                    node=node_name,
                    state=node_state,
                ),
            )

    return leader_node, member_nodes, offline_nodes


def change_cluster_leader(new_leader):
    leader, _, _ = get_cluster_members()
    if leader == new_leader:
        raise ClusterLeaderChangeToCurrent()

    say("Changing cluster leader to {new}.".format(new=new_leader))
    subprocess.check_call(['cfy', 'cluster', 'set-active', new_leader])

    # It can take a minute or more for failovers to occur. We'll allow for
    # five minutes in case of excessively slow changes on a large manager
    max_attempts = 100
    for attempt in range(1, max_attempts + 1):
        leader, _, offline = get_cluster_members()
        if offline:
            say(
                "Waiting for cluster members to come back online "
                "[{at}/{mx}]...".format(at=attempt, mx=max_attempts)
            )
        elif leader == new_leader:
            say("Cluster leader is now {new}.".format(new=new_leader))
            break
        else:
            say(
                "Waiting for {new} to become leader "
                "(current leader: {current}) [{at}/{mx}]...".format(
                    new=new_leader,
                    current=leader,
                    at=attempt,
                    mx=max_attempts,
                )
            )
        time.sleep(3)

    if leader != new_leader:
        raise ClusterLeaderChangeFailure(
            'Cluster leader did not become {new} within the timeout '
            'period.'.format(new=new_leader),
        )


def get_cfy_output(subcommand_and_args):
    command = ['cfy']
    command.extend(subcommand_and_args)

    command_output = subprocess.check_output(command)

    headers = []
    results = []
    dividers_found = 0
    for line in command_output.splitlines():
        if dividers_found == 3:
            # We reached the end of the output
            break

        line = line.strip()

        # Expecting something like:
        # Getting management services status... [ip=192.0.2.4]
        #
        # Services:
        # +--------------------------------+--------+
        # |            service             | status |
        # +--------------------------------+--------+
        # | Riemann                        |   up   |
        # | Celery Management              |   up   |
        # | Manager Rest-Service           |   up   |
        # | AMQP InfluxDB                  |   up   |
        # | RabbitMQ                       |   up   |
        # | Elasticsearch                  |   up   |
        # | Webserver                      |   up   |
        # | Logstash                       |   up   |
        # +--------------------------------+--------+
        if line.startswith('+--'):
            dividers_found += 1
        elif dividers_found == 1:
            # Headers are found after the first divider
            headers = [element.strip() for element in line.split('|')]
        elif dividers_found == 2:
            # Results are only shown after the second divider
            results.append([element.strip() for element in line.split('|')])

    return headers, results


def get_file_md5sum(file_path, manager_connection_string, ssh_port, keyfile):
    # Structure the command so that we return meaningful output in predictable
    # situations (where the path isn't a file (e.g. is a directory) or does
    # not exist), while still allowing failure for unexpected situations
    # (e.g. segfault on one of the commands)
    md5command = (
        'if [[ -f {path} ]]; '
        # sudo this so that we can avoid permissions issues
        '  then sudo md5sum {path}; '
        'elif [[ -e {path} ]]; '
        '  then echo -n "{notafile}"; '
        'else '
        '  echo -n "{notexisting}"; '
        'fi'.format(
            path=file_path,
            notafile="NOTAFILE",
            notexisting="DOESNOTEXIST",
        )
    )
    return ssh(
        manager_connection_string, ssh_port, keyfile, md5command
    ).split(' ')[0]


def get_local_md5sum(file_path):
    return subprocess.check_output(['md5sum', file_path]).split(' ')[0]


def md5sums_match(check_list, manager_connection_string, ssh_port, keyfile):
    matching = True
    for file_path, acceptable_md5sums in check_list.items():
        say('Checking md5sum for {path}'.format(path=file_path))
        md5sum = get_file_md5sum(
            file_path, manager_connection_string, ssh_port, keyfile
        )

        if md5sum not in acceptable_md5sums:
            matching = False
            if acceptable_md5sums == ["DOESNOTEXIST"]:
                message = (
                    '{file_path} should not exist, but file with md5sum '
                    '{md5sum} was found.'
                )
            else:
                message = (
                    '{file_path} has an incorrect md5sum. '
                    'md5sum was {md5sum}. '
                    'Allowed md5sums are {acceptable}'
                )

            sadly_say(message.format(
                file_path=file_path,
                md5sum=md5sum,
                acceptable=', '.join(acceptable_md5sums),
            ))

    return matching


def can_ssh(manager_connection_string, ssh_port, keyfile):
    try:
        ssh(manager_connection_string, ssh_port, keyfile, 'echo "SSH works."',
            stderr=subprocess.STDOUT)
        return True
    except subprocess.CalledProcessError as err:
        if err.returncode == 255 and 'usage' in err.output:
            sadly_say(
                "Invalid arguments to SSH command. Command was: {cmd}".format(
                    cmd=' '.join(err.cmd),
                )
            )
        else:
            sadly_say(
                'SSH attempt failed due to error:\n{err}\nCommand was: '
                '{cmd}'.format(
                    err=err.output,
                    cmd=' '.join(err.cmd),
                )
            )
        return False


def patch_command_available(manager_connection_string, ssh_port, keyfile):
    try:
        ssh(manager_connection_string, ssh_port, keyfile, 'which patch')
        return True
    except subprocess.CalledProcessError:
        return False


def ssh(manager_connection_string, ssh_port, keyfile, command,
        stderr=None):
    ssh_command = ['ssh', '-p', ssh_port]
    if keyfile:
        ssh_command.extend(['-i', keyfile])
    ssh_command.extend([manager_connection_string, command])
    return subprocess.check_output(ssh_command, stderr=stderr)


@contextmanager
def temporarily_own_directory(manager_connection_string, ssh_port, keyfile,
                              path):
    ssh(
        manager_connection_string,
        ssh_port,
        keyfile,
        'sudo chown $(whoami). ' + path,
    )
    try:
        yield
    finally:
        ssh(
            manager_connection_string,
            ssh_port,
            keyfile,
            'sudo chown root. ' + path,
        )


@contextmanager
def tempdir():
    tmp_path = tempfile.mkdtemp(prefix='patchify-')
    try:
        yield tmp_path
    finally:
        for tmp_file in os.listdir(tmp_path):
            os.unlink(os.path.join(tmp_path, tmp_file))
        os.rmdir(tmp_path)


def _scp(keyfile, ssh_port, source, dest, quiet=False):
    scp_command = ['scp', '-P', ssh_port]
    if keyfile:
        scp_command.extend(['-i', keyfile])
    scp_command.extend([source, dest])
    if quiet:
        subprocess.check_output(scp_command)
    else:
        subprocess.check_call(scp_command)


def upload(file_name, source_path, dest_path,
           manager_connection_string, ssh_port, keyfile,
           expected_md5sum=None):
    source = os.path.join(source_path, file_name)
    dest = '{mgr}:{dest_path}'.format(
        mgr=manager_connection_string,
        dest_path=dest_path,
    )
    say("Uploading {source} to {dest}".format(source=source, dest=dest))

    _scp(keyfile, ssh_port, source, dest)

    if not expected_md5sum:
        expected_md5sum = get_local_md5sum(source)

    uploaded_md5sum = get_file_md5sum(
        file_path=dest_path,
        manager_connection_string=manager_connection_string,
        ssh_port=ssh_port,
        keyfile=keyfile,
    )

    if not uploaded_md5sum == expected_md5sum:
        raise UploadError(
            'Uploaded file {file_name} had md5sum {actual}, but should '
            'have had {expected}.'.format(
                file_name=file_name,
                actual=uploaded_md5sum,
                expected=expected_md5sum,
            )
        )


def download(file_name, source_path, dest_path,
             manager_connection_string, ssh_port, keyfile, quiet=False):
    source = '{mgr}:{dest_path}'.format(
        mgr=manager_connection_string,
        dest_path=os.path.join(source_path, file_name),
    )
    dest = dest_path
    if not quiet:
        say("Downloading {source} to {dest}".format(source=source, dest=dest))

    _scp(keyfile, ssh_port, source, dest, quiet=quiet)

    original_md5sum = get_file_md5sum(
        file_path=os.path.join(source_path, file_name),
        manager_connection_string=manager_connection_string,
        ssh_port=ssh_port,
        keyfile=keyfile,
    )

    downloaded_md5sum = get_local_md5sum(dest)

    if not original_md5sum == downloaded_md5sum:
        raise DownloadError(
            'Downloaded file {file_path} had md5sum {actual}, but should '
            'have had {expected}.'.format(
                file_path=os.path.join(dest_path, file_name),
                actual=downloaded_md5sum,
                expected=original_md5sum,
            )
        )


def stop_services(manager_connection_string, ssh_port, services, keyfile):
    for service in services:
        ssh(
            manager_connection_string,
            ssh_port,
            keyfile,
            'sudo systemctl stop {service}'.format(service=service),
        )


def start_services(manager_connection_string, ssh_port, services, keyfile):
    # Ensure we're using any unit files we may have updated
    ssh(
        manager_connection_string,
        ssh_port,
        keyfile,
        'sudo systemctl daemon-reload',
    )
    for service in services:
        ssh(
            manager_connection_string,
            ssh_port,
            keyfile,
            'sudo systemctl start {service}'.format(service=service),
        )


def upload_and_verify_patch_files(manager_connection_string, ssh_port,
                                  patch_dir, patches, staging_dir,
                                  keyfile):
    for patch in patches:
        patch_name = patch['patch_file']
        expected_md5sum = patch['md5sum']

        upload(
            file_name=patch_name,
            source_path=patch_dir,
            dest_path=os.path.join(staging_dir, patch_name),
            expected_md5sum=expected_md5sum,
            manager_connection_string=manager_connection_string,
            ssh_port=ssh_port,
            keyfile=keyfile,
        )


def build_manager_version_output_string(version, community):
    version_output = version
    if community:
        version_output += ' community'
    else:
        version_output += ' premium'
    return version_output


def rollback(manager_connection_string, ssh_port, patch_definition,
             backup_root, delete_root, keyfile, skip_services):
    if skip_services:
        sadly_say(
            "Not ensuring services are stopped due to skip-services flag."
        )
    else:
        # Make sure all services are stopped, which might not be the case
        # depending on when we had to begin the rollback.
        sadly_say('Ensuring services are stopped for rollback.')
        stop_services(
            manager_connection_string,
            ssh_port,
            patch_definition['affected_services'],
            keyfile,
        )

    sadly_say('Rolling back file changes.')
    affected_files = get_list_of_affected_files(patch_definition['patches'])
    for affected_file in affected_files:
        backed_up_file = os.path.join(
            backup_root,
            affected_file.lstrip('/'),
        )
        delete_dest_parent = os.path.join(
            delete_root,
            os.path.split(affected_file.lstrip('/'))[0]
        )
        delete_dest = os.path.join(
            delete_root,
            affected_file.lstrip('/')
        )
        ssh(
            manager_connection_string,
            ssh_port,
            keyfile,
            'if sudo test -f {backup}; then '    # Only restore existing
            '  sudo cp {backup} {orig}; '        # and remove added files
            'else '
            '  sudo mkdir -p {delete_dest_parent} && '
            '  sudo mv {orig} {delete_dest}; '
            'fi'.format(
                backup=backed_up_file,
                orig=affected_file,
                delete_dest_parent=delete_dest_parent,
                delete_dest=delete_dest,
            ),
        )

    if skip_services:
        sadly_say("Not restarting services due to skip-services flag.")
    else:
        sadly_say('Starting services after rollback.')
        start_services(
            manager_connection_string,
            ssh_port,
            patch_definition['affected_services'],
            keyfile,
        )

    # TODO: We should check service state here

    sadly_say('Rollback complete.')


def get_list_of_affected_files(patches):
    affected_files = []
    for patch in patches:
        affected_files.extend(patch['destinations'])
    return set(affected_files)


def get_patch_storage_path(timestamp):
    # Don't use os.path.join in case this gets run on windows (the path itself
    # is on the manager, which will be Linux)
    return '{base}/{timestamp}'.format(
        base=BASE_MANAGER_PATCH_STORAGE_PATH,
        timestamp=timestamp,
    )


def abort_on_ssh_failure(manager_connection_string, ssh_port, keyfile,
                         cluster_node_name=None, quiet=False):
    if not quiet:
        say("Checking SSH.")
    if not can_ssh(manager_connection_string, ssh_port, keyfile):
        exit_message = (
            'Failed to SSH into manager with settings:\n'
            'Connection string (user@manager_ip): {conn_string}\n'
            'Port: {port}\n'
            'Keyfile: {key}\n'
            'Manager ssh profiles settings may be incorrect.\n'
            'To view current profile settings, run: '
            'cfy profiles show-current\n'
            'Then ensure that ssh_user is correct.\n'
        ).format(
            conn_string=manager_connection_string,
            port=ssh_port,
            key=keyfile or '<SSH default>',
        )
        if cluster_node_name:
            exit_message += (
                'cfy profiles set-cluster {node} --ssh-user '
                '<SSH username>\n'.format(node=cluster_node_name)
            )
        else:
            exit_message += (
                'cfy profiles set --ssh-user <SSH username>\n'
            )
        exit_message += (
            'If your ssh port and keyfiles are not default, you should '
            'also correct these settings:\n'
        )
        if cluster_node_name:
            exit_message += (
                'cfy profiles set-cluster {node} --ssh-key <SSH key path>\n'
                'cfy profiles set-cluster {node} --ssh-port '
                '<SSH port>'.format(node=cluster_node_name)
            )
        else:
            exit_message += (
                'cfy profiles set --ssh-key <SSH key path>\n'
                'cfy profiles set --ssh-port <SSH port>'
            )
        raise SSHCheckFailure(exit_message)


def abort_on_unhealthy_services():
    say("Checking service state.")
    unhealthy_services = get_unhealthy_services()
    if unhealthy_services:
        raise ServiceHealthFailure(
            'Not all services are healthy. '
            'Unhealthy services: {services}'.format(
                services=', '.join(unhealthy_services),
            ),
        )


def abort_on_missing_patch_command(manager_connection_string, ssh_port,
                                   keyfile, action, install_patch_command):
    say('Confirming patch command exists on server.')
    if not patch_command_available(manager_connection_string, ssh_port,
                                   keyfile):
        if install_patch_command:
            say("Patch command not found. Installing patch command.")
            ssh(
                manager_connection_string,
                ssh_port,
                keyfile,
                'sudo yum install -y patch',
            )
        else:
            exit_with_sadness(
                'Could not find "patch" command on server. '
                'On the manager, you may need to run: '
                '"sudo yum install -y patch" or call this script with '
                'the --install-patch-command flag.',
                action,
            )


def check_md5sums_before(patch_definition, manager_connection_string,
                         ssh_port, keyfile):
    say("Checking md5sums are as expected.")
    if not md5sums_match(patch_definition['md5sums_before'],
                         manager_connection_string,
                         ssh_port,
                         keyfile):
        raise MD5CheckFailure(
            'Unexpected md5sums found before starting patch installation.',
        )


def generate_patch_data_dir(patch_name, patch_definition, timestamp,
                            manager_connection_string, ssh_port, keyfile,
                            definition_file, action):
    patch_dirs = {}

    patch_path = get_patch_storage_path(timestamp)
    patch_dirs['patch_path'] = patch_path

    say("Creating patch directory in: {path}".format(path=patch_path))
    # We will also put some patch details to aid any later troubleshooting
    patch_info = 'attempting to apply ' + patch_name
    ssh(
        manager_connection_string,
        ssh_port,
        keyfile,
        'sudo mkdir -p ' + patch_path +
        ' && echo ' + patch_info +
        ' | sudo tee ' + patch_path + '/patch_info >/dev/null',
    )
    with temporarily_own_directory(manager_connection_string, ssh_port,
                                   keyfile, patch_path):
        source_path, file_name = os.path.split(definition_file)
        upload(
            file_name=file_name,
            source_path=source_path,
            dest_path=os.path.join(patch_path, 'patch_definition'),
            manager_connection_string=manager_connection_string,
            ssh_port=ssh_port,
            keyfile=keyfile,
        )

    patch_dirs['backup_root'] = '{base}/backups'.format(
        base=patch_path,
    )
    patch_dirs['delete_root'] = '{base}/deleted_files'.format(
        base=patch_path,
    )
    say(
        "Backing up affected files to {path}".format(
            path=patch_dirs['backup_root']
        )
    )
    ssh(
        manager_connection_string,
        ssh_port,
        keyfile,
        'sudo mkdir -p ' + patch_dirs['backup_root'],
    )
    ssh(
        manager_connection_string,
        ssh_port,
        keyfile,
        'sudo mkdir -p ' + patch_dirs['delete_root'],
    )
    affected_files = get_list_of_affected_files(patch_definition['patches'])
    for affected_file in affected_files:
        say(
            "Backing up {file_path} to {destination}".format(
                file_path=affected_file,
                destination=os.path.join(
                    patch_dirs['backup_root'],
                    affected_file.lstrip('/'),
                ),
            )
        )
        # We sudo (root) the copy because of user differences in different
        # versions of cloudify, but we probably should define this better
        # based on which version it's targetting.
        ssh(
            manager_connection_string,
            ssh_port,
            keyfile,
            'if sudo test -f {source}; then '  # Only if the file exists
            'sudo cp --parents {source} {destination}; fi'.format(
                source=affected_file,
                destination=patch_dirs['backup_root'],
            ),
        )

    if action == 'APPLYING PATCH':
        patch_dirs['staging_dir'] = '{base}/patch_files'.format(
            base=patch_path,
        )
        ssh(
            manager_connection_string,
            ssh_port,
            keyfile,
            'sudo mkdir -p ' + patch_dirs['staging_dir'],
        )
        with temporarily_own_directory(manager_connection_string, ssh_port,
                                       keyfile, patch_dirs['staging_dir']):
            patch_dir = os.path.join(os.path.dirname(definition_file),
                                     'patches')
            say("Uploading and verifying patch files.")
            try:
                upload_and_verify_patch_files(manager_connection_string,
                                              ssh_port,
                                              patch_dir,
                                              patch_definition['patches'],
                                              patch_dirs['staging_dir'],
                                              keyfile)
            except UploadError as err:
                raise UploadError(
                    'Failed to upload patch files: {err}'.format(
                        err=str(err),
                    ),
                )

    return patch_dirs


def generate_timestamp():
    # Avoiding : in timestamp to avoid needing to escape it for bash
    return datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S%z')


def possibly_stop_services(patch_definition, skip_services,
                           manager_connection_string, ssh_port, keyfile):
    if skip_services:
        say("Not stopping services due to skip-services flag.")
    else:
        say("Stopping services.")
        stop_services(manager_connection_string,
                      ssh_port,
                      patch_definition['affected_services'],
                      keyfile)


def possibly_start_services(patch_definition, skip_services,
                            manager_connection_string, ssh_port, keyfile):
    if skip_services:
        say("Not starting services due to skip-services flag.")
    else:
        say("Starting services.")
        start_services(manager_connection_string,
                       ssh_port,
                       patch_definition['affected_services'],
                       keyfile)


def check_md5sums_after(patch_definition,
                        manager_connection_string, ssh_port, keyfile,
                        backup_root, delete_root, skip_services):
    say("Checking post-patch md5sums.")
    if not md5sums_match(patch_definition['md5sums_after'],
                         manager_connection_string,
                         ssh_port,
                         keyfile):
        rollback(manager_connection_string, ssh_port, patch_definition,
                 backup_root, delete_root, keyfile, skip_services)
        raise MD5CheckFailure(
            'Unexpected md5sums found after patch installation.\n'
            'Changes rolled back.',
        )


def wait_for_healthy_services(patch_definition, backup_root, delete_root,
                              skip_services,
                              manager_connection_string, ssh_port, keyfile):
    say("Checking service state...")
    unhealthy_services = []
    max_attempts = 10
    for attempt in range(1, max_attempts + 1):
        if unhealthy_services:
            say(
                "Services still restarting [{at}/{mx}]...".format(
                    at=attempt,
                    mx=max_attempts,
                )
            )
            time.sleep(3)
        unhealthy_services = get_unhealthy_services()
    if unhealthy_services:
        rollback(manager_connection_string, ssh_port, patch_definition,
                 backup_root, delete_root, keyfile, skip_services)
        ServiceHealthFailure(
            'Services did not restart correctly. '
            'Unhealthy services: {services}\n'
            'Changes rolled back.'.format(
                services=', '.join(unhealthy_services),
            ),
        )


def patch_applier(definition_file, skip_services, install_patch_command,
                  skip_version_check):
    action = 'APPLYING PATCH'

    ssh_details = get_profile_ssh_details(action)

    say("Loading patch definition.")
    patch_definition = load_and_validate_patch_definition(definition_file,
                                                          action)

    patch_name = os.path.basename(definition_file)
    if patch_name.endswith('.json'):
        # Remove patch name extension
        patch_name = os.path.splitext(patch_name)[0]

    if not skip_version_check:
        say("Checking manager version is valid.")
        manager_version = get_manager_version()
        raw_version_output = manager_version.pop('raw')
        if manager_version is None:
            exit_with_sadness(
                'Could not get manager version! '
                'Are you in an environment with cfy using a profile for the '
                'manager you intend to patch?',
                action,
            )
        supported_versions = []
        if patch_definition['premium']:
            for version in patch_definition['manager_versions']:
                supported_versions.append({
                    'version': version,
                    'community': False,
                })
        if patch_definition['community']:
            for version in patch_definition['manager_versions']:
                supported_versions.append({
                    'version': version,
                    'community': True,
                })
        if manager_version not in supported_versions:
            manager_version_output = manager_version['version']
            try:
                manager_version_output = build_manager_version_output_string(
                    version=manager_version['version'],
                    community=manager_version['community'],
                )
            except Exception as err:
                exit_with_sadness(
                    'Could not determine manager version.\n'
                    'Version check output was: {raw_output}\n'
                    'Error was: {err}'.format(
                        raw_output=raw_version_output,
                        err=str(err),
                    ),
                    action,
                )
            supported_versions_output = ', '.join([
                build_manager_version_output_string(
                    version=version['version'],
                    community=version['community'],
                )
                for version in supported_versions
            ])
            exit_with_sadness(
                'Manager version mismatch. '
                'Manager is {manager_version}, but patch only supports '
                '{supported_versions}'.format(
                    manager_version=manager_version_output,
                    supported_versions=supported_versions_output,
                ),
                action,
            )

    if len(ssh_details) > 1:
        say("Attempting to apply patch to cluster.")
        leader, members, offline = get_cluster_members()

        if offline:
            exit_with_sadness(
                (
                    'Cluster members {offline} are offline. Please ensure '
                    'cluster is healthy before attempting to apply the '
                    'patch.'.format(
                        offline=', '.join(offline),
                    )
                ),
                action,
            )

        # We will process the members in arbitrary order, followed by the
        # current leader, as we need to switch the leader after each
        # application to ensure services still work without the patch
        members.append(leader)
        member_ssh = {
            entry['cluster_node_name']: entry for entry in ssh_details
        }
        say("Validating SSH connectivity to members.")
        for member in members:
            try:
                abort_on_ssh_failure(
                    member_ssh[member]['connection_string'],
                    member_ssh[member]['ssh_port'],
                    member_ssh[member]['key_path'],
                    cluster_node_name=member,
                )
                abort_on_missing_patch_command(
                    member_ssh[member]['connection_string'],
                    member_ssh[member]['ssh_port'],
                    member_ssh[member]['key_path'],
                    action,
                    install_patch_command,
                )
            except SSHCheckFailure as err:
                exit_with_sadness(str(err), action)

        apply_not_required = 0

        for member in members:
            applied_to_this_node = True
            say(
                "Attempting to apply patch to {member}".format(
                    member=member,
                )
            )
            try:
                rollback_args = apply_patch(
                    patch_definition,
                    patch_name,
                    definition_file,
                    # Don't play with services or the cluster will get unhappy
                    True,
                    member_ssh[member],
                    action,
                )
            except (
                ServiceHealthFailure,
                PatchAlreadyApplied,
                PatchApplyFailed,
                PatchDefinitionParseFailure,
                MD5CheckFailure,
                RegistryGenerationFailure,
            ) as err:
                if isinstance(err, PatchAlreadyApplied):
                    say(
                        "Patch did not need applying to {member}.".format(
                            member=member,
                        )
                    )
                    applied_to_this_node = False
                    apply_not_required += 1
                else:
                    exit_with_sadness(
                        (
                            'Failed to apply patch to node.\n'
                            'Aborting patch application attempt.'
                        ),
                        action,
                    )
            # Confirming health of cluster member
            try:
                if applied_to_this_node:
                    # If no other nodes needed modifying, the leader will
                    # still be the original leader, so we must switch away
                    # then back
                    if member == leader \
                       and apply_not_required == len(members) - 1:
                        say(
                            "Switching away from old leader temporarily to "
                            "confirm services are healthy after modification."
                        )
                        change_cluster_leader(members[0])
                    change_cluster_leader(member)
            except ClusterLeaderChangeFailure as err:
                sadly_say(
                    "Failed to switch to {member}.\n"
                    "Attempting to roll back patch on this manager.".format(
                        member=member,
                    )
                )
                rollback(*rollback_args)
                exit_with_sadness(
                    (
                        "Rolled back {member}. Patch has not been fully "
                        "applied to cluster.\n"
                        "Please verify cluster healthy before re-attempting "
                        "this action."
                    ),
                    action,
                )
            except ClusterLeaderChangeToCurrent as err:
                exit_with_sadness(
                    (
                        "Failure attempting to change leader to {member}, "
                        "because this node was already the leader.\n"
                        "Cluster appears to be undergoing unexpected leader "
                        "changes while patching.".format(
                            member=member,
                        )
                    ),
                    action,
                )
        if apply_not_required == len(members):
            exit_with_sadness(
                'Patch was already installed on all nodes in the cluster!',
                action,
            )
        else:
            say("Patch applied to cluster.")
    else:
        say("Attempting to apply patch to manager.")
        ssh_details = ssh_details[0]
        try:
            abort_on_unhealthy_services()
            abort_on_ssh_failure(
                ssh_details['connection_string'],
                ssh_details['ssh_port'],
                ssh_details['key_path'],
            )
            abort_on_missing_patch_command(
                ssh_details['connection_string'],
                ssh_details['ssh_port'],
                ssh_details['key_path'],
                action,
                install_patch_command,
            )
            apply_patch(
                patch_definition,
                patch_name,
                definition_file,
                skip_services,
                ssh_details,
                action,
            )
        except (
            ServiceHealthFailure,
            SSHCheckFailure,
            PatchApplyFailed,
            PatchAlreadyApplied,
            PatchDefinitionParseFailure,
            MD5CheckFailure,
            RegistryGenerationFailure,
        ) as err:
            exit_with_sadness(str(err), action)


def apply_patch(patch_definition, patch_name, definition_file, skip_services,
                ssh_details, action):
    manager_connection_string = ssh_details['connection_string']
    keyfile = ssh_details['key_path']
    ssh_port = ssh_details['ssh_port']

    say("Retrieving patch registry information.")
    registry = download_patch_registry(manager_connection_string,
                                       ssh_port, keyfile)

    say("Checking whether patch has already been applied.")
    definition_md5sum = get_local_md5sum(definition_file)
    patch_id = generate_patch_id(patch_name, definition_md5sum)
    current_patches = registry.get('current_patches')
    this_patch = None
    for patch in current_patches:
        if patch['patch_id'] == patch_id:
            say('New patch ID located.')
            this_patch = patch
            break
    if this_patch is not None:
        raise PatchAlreadyApplied(
            'Existing patch found with ID {patch_id}.\n'
            'This patch appears to already be installed.'.format(
                patch_id=patch_id,
            )
        )

    check_md5sums_before(patch_definition, manager_connection_string,
                         ssh_port, keyfile)

    timestamp = generate_timestamp()
    say("Patching timestamp is: {timestamp}".format(timestamp=timestamp))
    patch_dirs = generate_patch_data_dir(patch_name, patch_definition,
                                         timestamp,
                                         manager_connection_string, ssh_port,
                                         keyfile, definition_file, action)
    backup_root = patch_dirs['backup_root']
    delete_root = patch_dirs['delete_root']
    staging_dir = patch_dirs['staging_dir']

    possibly_stop_services(patch_definition, skip_services,
                           manager_connection_string, ssh_port, keyfile)

    say("Applying patches.")
    for patch in patch_definition['patches']:
        for destination in patch['destinations']:
            say(
                'Applying {patch} to {target}'.format(
                    patch=patch['patch_file'],
                    target=destination,
                )
            )
            try:
                ssh(
                    manager_connection_string,
                    ssh_port,
                    keyfile,
                    'sudo patch {destination} {patch}'.format(
                        destination=destination,
                        patch=os.path.join(
                            staging_dir,
                            patch['patch_file'],
                        ),
                    )
                )
            except subprocess.CalledProcessError as err:
                rollback(manager_connection_string, ssh_port,
                         patch_definition, backup_root, delete_root, keyfile,
                         skip_services)
                raise PatchApplyFailed(
                    'Failed to apply patch {patch} to {target}\n'
                    'Changes rolled back.\n'
                    'Error was: {err}\n'.format(
                        patch=patch['patch_file'],
                        target=destination,
                        err=str(err),
                    ),
                )

    check_md5sums_after(patch_definition,
                        manager_connection_string, ssh_port, keyfile,
                        backup_root, delete_root, skip_services)

    possibly_start_services(patch_definition, skip_services,
                            manager_connection_string, ssh_port, keyfile)

    wait_for_healthy_services(patch_definition, backup_root, delete_root,
                              skip_services, manager_connection_string,
                              ssh_port, keyfile)

    say("Patch installed and services healthy.")

    say("Updating patch information and registry.")
    patch_info = "applied " + patch_name
    ssh(
        manager_connection_string,
        ssh_port,
        keyfile,
        'echo ' + patch_info +
        ' | sudo tee ' + patch_dirs['patch_path'] + '/patch_info >/dev/null',
    )
    add_patch_to_registry(registry, patch_id, timestamp, patch_name,
                          patch_dirs['patch_path'], patch_definition,
                          manager_connection_string, ssh_port, keyfile)

    say('Patch applied!')

    # Return arguments required for rollback to allow rollback in cluster
    # patching
    return (
        manager_connection_string,
        ssh_port,
        patch_definition,
        backup_root,
        delete_root,
        keyfile,
        skip_services,
    )


def patch_remover(target_patch_id, skip_services):
    action = 'REMOVING PATCH'

    ssh_details = get_profile_ssh_details(action)

    if len(ssh_details) > 1:
        say("Attempting to remove patch from cluster.")
        leader, members, offline = get_cluster_members()

        if offline:
            exit_with_sadness(
                (
                    'Cluster members {offline} are offline. Please ensure '
                    'cluster is healthy before attempting to remove the '
                    'patch.'.format(
                        offline=', '.join(offline),
                    )
                ),
                action,
            )

        # We will process the members in arbitrary order, followed by the
        # current leader, as we need to switch the leader after each removal
        # to ensure services still work without the patch
        members.append(leader)
        member_ssh = {
            entry['cluster_node_name']: entry for entry in ssh_details
        }

        say("Validating SSH connectivity to members.")
        for member in members:
            try:
                abort_on_ssh_failure(
                    member_ssh[member]['connection_string'],
                    member_ssh[member]['ssh_port'],
                    member_ssh[member]['key_path'],
                    cluster_node_name=member,
                )
            except SSHCheckFailure as err:
                exit_with_sadness(str(err), action)

        removal_not_required = 0

        for member in members:
            removed_from_this_node = True
            say(
                "Attempting to remove patch from {member}".format(
                    member=member,
                )
            )
            try:
                rollback_args = remove_patch(
                    target_patch_id,
                    # Don't play with services or the cluster will get unhappy
                    True,
                    member_ssh[member],
                    action,
                )
            except (
                ServiceHealthFailure,
                SSHCheckFailure,
                PatchNotFound,
                PatchRemovalBlocked,
                PatchDefinitionParseFailure,
                InvalidBackup,
                MD5CheckFailure,
                RegistryGenerationFailure,
            ) as err:
                if isinstance(err, PatchNotFound):
                    say(
                        "Patch did not need removing from {member}.".format(
                            member=member,
                        )
                    )
                    removed_from_this_node = False
                    removal_not_required += 1
                else:
                    exit_with_sadness(
                        (
                            'Failed to remove patch from node.\n'
                            'Aborting patch removal attempt.'
                        ),
                        action,
                    )
            # Confirming health of cluster member
            try:
                if removed_from_this_node:
                    # If no other nodes needed modifying, the leader will
                    # still be the original leader, so we must switch away
                    # then back
                    if member == leader \
                       and removal_not_required == len(members) - 1:
                        say(
                            "Switching away from old leader temporarily to "
                            "confirm services are healthy after modification."
                        )
                        change_cluster_leader(members[0])
                    change_cluster_leader(member)
            except ClusterLeaderChangeFailure as err:
                sadly_say(
                    "Failed to switch to {member}.\n"
                    "Attempting to roll back patch on this manager.".format(
                        member=member,
                    )
                )
                rollback(*rollback_args)
                exit_with_sadness(
                    (
                        "Rolled back {member}. Patch has not been fully "
                        "removed from cluster.\n"
                        "Please verify cluster healthy before re-attempting "
                        "this action."
                    ),
                    action,
                )
            except ClusterLeaderChangeToCurrent as err:
                exit_with_sadness(
                    (
                        "Failure attempting to change leader to {member}, "
                        "because this node was already the leader.\n"
                        "Cluster appears to be undergoing unexpected leader "
                        "changes while patching.".format(
                            member=member,
                        )
                    ),
                    action,
                )
        if removal_not_required == len(members):
            exit_with_sadness(
                'Patch was not found on any nodes in the cluster!',
                action,
            )
        else:
            say("Patch removed from cluster.")
    else:
        say("Attempting to remove patch from manager.")
        ssh_details = ssh_details[0]
        try:
            abort_on_unhealthy_services()
            abort_on_ssh_failure(
                ssh_details['connection_string'],
                ssh_details['ssh_port'],
                ssh_details['key_path'],
            )
            remove_patch(
                target_patch_id,
                skip_services,
                ssh_details,
                action,
            )
        except (
            ServiceHealthFailure,
            SSHCheckFailure,
            PatchNotFound,
            PatchRemovalBlocked,
            PatchDefinitionParseFailure,
            InvalidBackup,
            MD5CheckFailure,
            RegistryGenerationFailure,
        ) as err:
            exit_with_sadness(str(err), action)


def remove_patch(target_patch_id, skip_services, ssh_details, action):
    manager_connection_string = ssh_details['connection_string']
    keyfile = ssh_details['key_path']
    ssh_port = ssh_details['ssh_port']

    say("Retrieving patch registry information.")
    try:
        registry = download_patch_registry(manager_connection_string,
                                           ssh_port, keyfile)
    except PatchRegistryParseFailure as err:
        exit_with_sadness(str(err), action)

    say("Getting target patch information...")
    current_patches = registry.get('current_patches')
    target_patch = None
    for patch in current_patches:
        if patch['patch_id'] == target_patch_id:
            say('Target patch information located.')
            target_patch = patch
            break
    if target_patch is None:
        all_patch_ids = [patch['patch_id'] for patch in current_patches]
        raise PatchNotFound(
            'Could not locate patch with ID {patch_id}.\n'
            'Currently installed patches were: {patches}\n'
            'For more details, run {prog} with the list subcommand.'.format(
                patch_id=target_patch_id,
                patches=', '.join(all_patch_ids),
                prog=sys.argv[0],
            ),
        )

    say("Checking for blocking patches...")
    blockers = get_uninstall_blockers(current_patches).get(target_patch_id)
    if blockers is not None:
        raise PatchRemovalBlocked(
            'Could not uninstall patch with ID {patch_id}.\n'
            'Patch uninstall blocked by patches: {blockers}'.format(
                patch_id=target_patch_id,
                blockers=', '.join(blockers),
            ),
        )
    say("No blockers, patch is valid candidate for uninstall.")

    say("Retrieving old patch definition.")
    with tempdir() as tmp_path:
        local_patch_definition = os.path.join(tmp_path, 'patch_definition')
        download(
            file_name='patch_definition',
            source_path=patch['patch_directory'],
            dest_path=local_patch_definition,
            manager_connection_string=manager_connection_string,
            ssh_port=ssh_port,
            keyfile=keyfile,
        )
        try:
            with open(local_patch_definition) as def_handle:
                old_definition = json.load(def_handle)
        except ValueError:
            raise PatchDefinitionParseFailure(
                'Could not parse old patch definition retrieved from '
                '{patch_def_location}.'.format(
                    patch_def_location=os.path.join(
                        patch['patch_directory'], 'patch_definition',
                    ),
                ),
            )

    say("Building removal definition.")
    backup_path = os.path.join(
        patch['patch_directory'],
        'backups',
    )
    new_definition = {
        'md5sums_before': old_definition['md5sums_after'],
        'md5sums_after': old_definition['md5sums_before'],
        'patches': [
            {'destinations': old_patch['destinations']}
            for old_patch in old_definition['patches']
        ],
        'affected_services': old_definition['affected_services'],
        'removed_patch_backups_path': backup_path,
    }

    say("Checking for correct files to restore.")
    files_to_delete = []
    files_to_restore = []
    for target_file, valid_md5sums in new_definition['md5sums_after'].items():
        target_backup_path = os.path.join(
            backup_path,
            target_file.lstrip('/'),
        )

        backup_md5sum = get_file_md5sum(
            file_path=target_backup_path,
            manager_connection_string=manager_connection_string,
            ssh_port=ssh_port,
            keyfile=keyfile,
        )
        if backup_md5sum not in valid_md5sums:
            raise InvalidBackup(
                'File to be restored does not match any acceptable MD5sums.\n'
                'File to be restored was: {target_file}\n'
                'MD5sum was "{backup_md5sum}", but valid MD5sums were: '
                '{valid}'.format(
                    target_file=target_backup_path,
                    backup_md5sum=backup_md5sum,
                    valid=', '.join(valid_md5sums),
                ),
            )

        if backup_md5sum == 'DOESNOTEXIST':
            files_to_delete.append(target_file)
        else:
            files_to_restore.append((target_backup_path, target_file))

    say("Validating current files.")
    check_md5sums_before(new_definition, manager_connection_string,
                         ssh_port, keyfile)

    possibly_stop_services(new_definition, skip_services,
                           manager_connection_string, ssh_port, keyfile)

    say("Deploying removal directory.")
    with tempdir() as tmp_dir:
        temporary_definition_file = os.path.join(tmp_dir, 'patch_definition')
        with open(temporary_definition_file, 'w') as def_handle:
            def_handle.write(json.dumps(new_definition))
        patch_dirs = generate_patch_data_dir(
            patch_name='remove-' + patch['patch_name'],
            patch_definition=new_definition,
            timestamp=generate_timestamp(),
            manager_connection_string=manager_connection_string,
            ssh_port=ssh_port,
            keyfile=keyfile,
            definition_file=temporary_definition_file,
            action=action,
        )

    if files_to_delete:
        say("Removing files added by target patch.")
        for delete_file in files_to_delete:
            ssh(
                manager_connection_string,
                ssh_port,
                keyfile,
                'sudo rm -f {delete_file} '.format(
                    delete_file=delete_file,
                )
            )
            delete_file_base, extension = os.path.splitext(delete_file)
            if extension == '.py':
                ssh(
                    manager_connection_string,
                    ssh_port,
                    keyfile,
                    'sudo rm -f {pyartefacts}'.format(
                        pyartefacts=delete_file_base + '.py[co]',
                    )
                )

    if files_to_restore:
        say("Restoring files modified by target patch.'")
        for file_backup, target_file in files_to_restore:
            ssh(
                manager_connection_string,
                ssh_port,
                keyfile,
                'sudo cp {backup} {restored}'.format(
                    backup=file_backup,
                    restored=target_file,
                )
            )
            restore_file_base, extension = os.path.splitext(target_file)
            if extension == '.py':
                ssh(
                    manager_connection_string,
                    ssh_port,
                    keyfile,
                    'sudo rm -f {pyartefacts}'.format(
                        pyartefacts=restore_file_base + '.py[co]',
                    )
                )

    backup_root = patch_dirs['backup_root']
    delete_root = patch_dirs['delete_root']
    check_md5sums_after(new_definition,
                        manager_connection_string, ssh_port, keyfile,
                        backup_root, delete_root, skip_services)

    possibly_start_services(new_definition, skip_services,
                            manager_connection_string, ssh_port, keyfile)

    wait_for_healthy_services(new_definition, backup_root, delete_root,
                              skip_services,
                              manager_connection_string, ssh_port, keyfile)

    say("Patch removed, updating registry.")

    remove_patch_from_registry(registry, target_patch_id,
                               manager_connection_string, ssh_port, keyfile)

    say("Patch removed successfully.")

    # Return arguments required for rollback to allow rollback in cluster
    # patching
    return (
        manager_connection_string,
        ssh_port,
        new_definition,
        backup_root,
        delete_root,
        keyfile,
        skip_services,
    )


def generate_patch_id(patch_name, definition_md5sum):
    return patch_name + '-' + definition_md5sum


def add_patch_to_registry(registry, patch_id, timestamp,
                          patch_name, patch_path, patch_definition,
                          manager_connection_string, ssh_port, keyfile):
    current_patches = registry.get('current_patches', [])

    affected_files = patch_definition['md5sums_after'].keys()

    block_uninstall = []
    for existing_patch in current_patches:
        if any(affected_file in existing_patch['modified_files']
               for affected_file in affected_files):
            block_uninstall.append(existing_patch['patch_id'])

    new_patch = {
        'patch_id': patch_id,
        'patch_name': patch_name,
        'timestamp': timestamp,
        'patch_directory': patch_path,
        'modified_files': patch_definition['md5sums_after'],
        'blocks_uninstall_of': block_uninstall,
        'description': patch_definition.get('description'),
    }

    current_patches.append(new_patch)

    registry['current_patches'] = current_patches

    update_registry(registry, manager_connection_string, ssh_port, keyfile)


def update_registry(registry, manager_connection_string, ssh_port, keyfile):
    with tempdir() as tmp_path:
        local_reg_path = os.path.join(tmp_path, PATCH_REGISTRY_FILE_NAME)

        try:
            with open(local_reg_path, 'w') as local_reg_handle:
                local_reg_handle.write(json.dumps(registry))
        except Exception as err:
            raise RegistryGenerationFailure(
                'FAILED TO GENERATE NEW REGISTRY!\n'
                'PATCH STATE HAS BEEN UPDATED BUT REGISTRY IS INCORRECT!\n'
                'Error was: {err}'.format(err=err),
            )

        with temporarily_own_directory(manager_connection_string, ssh_port,
                                       keyfile,
                                       BASE_MANAGER_PATCH_STORAGE_PATH):
            upload(
                file_name=PATCH_REGISTRY_FILE_NAME,
                source_path=tmp_path,
                dest_path=PATCH_REGISTRY_PATH,
                manager_connection_string=manager_connection_string,
                ssh_port=ssh_port,
                keyfile=keyfile,
            )


def remove_patch_from_registry(registry, patch_id, manager_connection_string,
                               ssh_port, keyfile):
    current_patches = []
    for patch in registry['current_patches']:
        if patch['patch_id'] == patch_id:
            continue
        else:
            current_patches.append(patch)

    registry['current_patches'] = current_patches

    update_registry(registry, manager_connection_string, ssh_port, keyfile)


def get_uninstall_blockers(patches):
    blockers = {}
    for patch in patches:
        for blocked in patch.get('blocks_uninstall_of', []):
            if blocked not in blockers:
                blockers[blocked] = []
            blockers[blocked].append(patch['patch_id'])
    return blockers


def list_patches(json_output):
    action = 'LISTING PATCHES'

    ssh_details = get_profile_ssh_details(action)

    if len(ssh_details) > 1:
        say("Determining cluster leader.")
        leader, _, _ = get_cluster_members()
    else:
        leader = None

    manager_patches = {}

    for mgr_ssh_details in ssh_details:
        manager_connection_string = mgr_ssh_details['connection_string']
        keyfile = mgr_ssh_details['key_path']
        ssh_port = mgr_ssh_details['ssh_port']
        cluster_node_name = mgr_ssh_details['cluster_node_name']

        if not json_output:
            if cluster_node_name:
                say(
                    "Getting installed patch details from {node}...".format(
                        node=cluster_node_name,
                    )
                )
            else:
                say("Getting installed patch details from manager...")
        try:
            abort_on_ssh_failure(manager_connection_string, ssh_port, keyfile,
                                 cluster_node_name=cluster_node_name,
                                 quiet=json_output)
        except SSHCheckFailure as err:
            exit_with_sadness(str(err), action)
        try:
            registry = download_patch_registry(manager_connection_string,
                                               ssh_port, keyfile,
                                               quiet=json_output)
        except PatchRegistryParseFailure as err:
            exit_with_sadness(str(err), action)
        manager_patches[cluster_node_name] = registry.get('current_patches',
                                                          [])

    if json_output:
        if leader:
            say(json.dumps(manager_patches))
        else:
            say(json.dumps(manager_patches[None]))
    else:
        for patches in manager_patches.values():
            uninstall_blocked = get_uninstall_blockers(patches)
            for patch in patches:
                if patch['patch_id'] in uninstall_blocked:
                    patch['blockers'] = uninstall_blocked[patch['patch_id']]
                else:
                    patch['blockers'] = []

        # Determine if any of the cluster nodes have different patches
        if leader:
            node_groups = []
            for node, patches in manager_patches.items():
                matches_others = False
                # If we compare with timestamps included it is easy to get
                # false positives
                node_timestamps = [
                    patch.pop('timestamp') for patch in patches
                ]
                # Strip out patch directories as they contain the timestamp
                # and we don't display them in 'friendly' output anyway
                for patch in patches:
                    patch.pop('patch_directory')
                for grp_nodes, grp_patches, grp_timestamps in node_groups:
                    if patches == grp_patches:
                        grp_nodes.append(node)
                        grp_timestamps.append(node_timestamps)
                        matches_others = True
                if not matches_others:
                    node_groups.append(([node], patches, [node_timestamps]))

            # Sort the groups, putting the most frequently found patch
            # combinations first, then by amount of patches if there is a
            # collision
            node_groups.sort(key=lambda x: (len(x[0]), len(x[1])),
                             reverse=True)
            # And then put the group with the leader at the top of the list
            for idx, entry in enumerate(node_groups):
                if leader in entry[0]:
                    leader_group_pos = idx
                    break
            node_groups.insert(
                0,
                node_groups.pop(leader_group_pos),
            )

            # Merge the timestamps back in
            for grp_nodes, grp_patches, grp_timestamps in node_groups:
                for patch_num in range(len(grp_patches)):
                    grp_patches[patch_num]['timestamp'] = [
                        grp_timestamps[node_num][patch_num] +
                        '(' + grp_nodes[node_num] + ')'
                        for node_num in range(len(grp_nodes))
                    ]

            display_patches = node_groups[0][1]
            differences = node_groups[1:]
        else:
            display_patches = manager_patches[None]
            differences = []

        if display_patches:
            say('')
            if leader:
                say("Cluster patches:")
            display_patch_listing(display_patches)
        else:
            if leader:
                say('')
                say("Cluster patches:")
                if differences:
                    say("No installed patches found on leader.")
                else:
                    say("No installed patches found on cluster.")
            else:
                say("No installed patches found on manager.")

        for different_nodes, different_patches, _ in differences:
            sadly_say('')
            if len(different_nodes) > 1:
                sadly_say('Replicas {nodes} have different patches:'.format(
                    nodes=', '.join(different_nodes),
                ))
            else:
                sadly_say('Replica {node} has different patches:'.format(
                    node=different_nodes[0],
                ))
            if different_patches:
                display_patch_listing(different_patches,
                                      output_method=sadly_say)
            else:
                sadly_say('No patches installed.')
            sadly_say('')

        if differences:
            sadly_say(
                "Please try to resolve patch issues by applying or removing "
                "patches from the cluster."
            )
            sys.exit(1)


def display_patch_listing(patches, output_method=say):
    for patch in patches:
        if patch['blockers']:
            uninstall_restrictions = (
                ' Uninstall blocked by: {blockers}'.format(
                    blockers=', '.join(patch['blockers']),
                )
            )
        else:
            uninstall_restrictions = ''

        if patch.get('description'):
            description = (
                ' Patch description: {description}'.format(
                    description=patch['description'],
                )
            )
        else:
            description = ''

        if isinstance(patch['timestamp'], list):
            display_timestamp = ', '.join(patch['timestamp'])
        else:
            display_timestamp = patch['timestamp']

        output_method(
            'Patch {name} (ID: {patch_id}) was installed on '
            '{timestamp}.{description}{uninstall_restrictions}'.format(
                name=patch['patch_name'],
                patch_id=patch['patch_id'],
                timestamp=display_timestamp,
                uninstall_restrictions=uninstall_restrictions,
                description=description,
            )
        )


def download_patch_registry(manager_connection_string, ssh_port, keyfile,
                            quiet=False):
    patch_registry_md5 = get_file_md5sum(
        PATCH_REGISTRY_PATH,
        manager_connection_string,
        ssh_port,
        keyfile,
    )
    if patch_registry_md5 == 'DOESNOTEXIST':
        registry = {
            'current_patches': [],
        }
    else:
        with tempdir() as tmp_path:
            local_reg_path = os.path.join(tmp_path, PATCH_REGISTRY_FILE_NAME)

            download(
                file_name=PATCH_REGISTRY_FILE_NAME,
                source_path=BASE_MANAGER_PATCH_STORAGE_PATH,
                dest_path=local_reg_path,
                manager_connection_string=manager_connection_string,
                ssh_port=ssh_port,
                keyfile=keyfile,
                quiet=quiet,
            )

            try:
                with open(local_reg_path) as local_reg_handle:
                    registry = json.load(local_reg_handle)
            except ValueError:
                raise PatchRegistryParseFailure(
                    'Could not parse existing patch registry.',
                )
    return registry


def add_apply_and_remove_args(parser):
    parser.add_argument(
        '-s', '--skip-services',
        help=(
            'Set this flag to not restart services. This is only to be used '
            'when instructed by Cloudify support.'
        ),
        action='store_true',
        default=False,
        required=False,
    )
    parser.add_argument(
        '-S', '--skip-version-check',
        help=(
            'Set this flag to not check manager version. This is only to be '
            'used when instructed by Cloudify support.'
        ),
        action='store_true',
        default=False,
        required=False,
    )


def main():
    parser = argparse.ArgumentParser(
        description='Apply a patch to a cloudify manager.',
    )

    subparsers = parser.add_subparsers(help='Patch action',
                                       dest='action')

    list_patch_args = subparsers.add_parser('list',
                                            help='List installed patches')
    list_patch_args.add_argument(
        '-j', '--json',
        help=(
            'Whether to output the patch list in json. '
            'If true, a json dict mapping timestamps to patch names will be '
            'the output.'
        ),
        action='store_true',
        default=False,
        required=False,
    )

    apply_patch_args = subparsers.add_parser('apply',
                                             help='Apply a patch')
    apply_patch_args.add_argument(
        '-p', '--patch_file',
        help=(
            'The patch file to use. Note that for POC purposes this is just '
            'a definition file, but this should probably be changed to a zip '
            'for real world use (so that it can work on Windows as well as '
            'Linux.'
        ),
        required=True,
    )
    apply_patch_args.add_argument(
        '-P', '--install-patch-command',
        help=(
            'Set this flag to yum install the patch command if it is not '
            'present.'
        ),
        action='store_true',
        default=False,
        required=False,
    )
    add_apply_and_remove_args(apply_patch_args)

    remove_patch_args = subparsers.add_parser('remove',
                                              help='Remove a patch')
    remove_patch_args.add_argument(
        '-p', '--patch-id',
        help=(
            'The ID of the patch to remove. This can be obtained using the '
            'list subcommand.'
        ),
        required=True,
    )
    add_apply_and_remove_args(remove_patch_args)

    args = parser.parse_args()

    if args.action == 'apply':
        patch_applier(args.patch_file, args.skip_services,
                      args.install_patch_command, args.skip_version_check)
    elif args.action == 'remove':
        patch_remover(args.patch_id, args.skip_services)
    elif args.action == 'list':
        list_patches(args.json)
    else:
        # You can't get here without messing about with the code above
        raise RuntimeError('Invalid action specified in parser.')


if __name__ == '__main__':
    main()
